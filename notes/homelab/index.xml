<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Homelab on Vladimir Varankin</title><link>https://vladimir.varank.in/notes/homelab/</link><description>Recent content in Homelab on Vladimir Varankin</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sat, 10 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://vladimir.varank.in/notes/homelab/index.xml" rel="self" type="application/rss+xml"/><item><title>Self-signed certificates with k3s and cert-manager</title><link>https://vladimir.varank.in/notes/2021/07/self-signed-certificates-with-k3s-and-cert-manager/</link><pubDate>Sat, 10 Jul 2021 00:00:00 +0000</pubDate><guid>https://vladimir.varank.in/notes/2021/07/self-signed-certificates-with-k3s-and-cert-manager/</guid><description>&lt;p>At least for now, my homelab cluster (&lt;a href="https://vladimir.varank.in/notes/2020/01/raspi-ubuntu-arm64-k3s/">4x Raspberry Pi, k3s, etc&lt;/a>) is available only for the devices &lt;a href="https://vladimir.varank.in/notes/2021/04/wireless-to-ethernet-island-for-homelab-cluster-ipv6-ndp-proxy-and-mdns-reflector/">on my local network&lt;/a>, inside a custom DNS zone &lt;code>k8s.pi.home&lt;/code>. I don&amp;rsquo;t think there are practical reasons to run anything with HTTPS in that setup, but there are cases, like browser extensions, where it&amp;rsquo;s required.&lt;/p>
&lt;p>Turned out, in 2021, it&amp;rsquo;s fairly straight forward to set up a Certificate Authority (CA), that will issue TLS certificates to &amp;ldquo;secure&amp;rdquo; the ingress resources. At least, it&amp;rsquo;s way simpler comparing to how I remember it was back in the days. All thanks to &lt;a href="https://cert-manager.io">cert-manager&lt;/a> and some YAML.&lt;/p>
&lt;p>First thing is to install cert-manager to the cluster. &lt;a href="https://rancher.com/docs/k3s/latest/en/helm/">k3s comes with helm-controller&lt;/a>, that gives us a way to manage helm charts with Custom Resource Definitions (CRD). The following manifest defines a new namespace, and a resource of a kind &lt;code>HelmChart&lt;/code>, to install cert-manager inside this namespace:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Namespace
metadata:
 name: cert-manager

---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
 name: cert-manager
 namespace: kube-system
spec:
 chart: cert-manager
 repo: https://charts.jetstack.io
 targetNamespace: cert-manager
 valuesContent: |-
 installCRDs: true
 prometheus:
 enabled: true
 servicemonitor:
 enabled: true
&lt;/code>&lt;/pre>&lt;p>After applying the manifest above — &lt;code>kubectl apply -f cert-manager.yml&lt;/code> — define a self-signed certificate, which is used to bootstrap a CA:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
 name: selfsigned-cluster-issuer
spec:
 selfSigned: {}

---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
 name: selfsigned-ca
spec:
 isCA: true
 commonName: selfsigned-ca
 secretName: selfsigned-ca-root-secret
 privateKey:
 algorithm: ECDSA
 size: 256
 issuerRef:
 name: selfsigned-cluster-issuer
 kind: ClusterIssuer
 group: cert-manager.io

---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
 name: selfsigned-issuer
spec:
 ca:
 secretName: selfsigned-ca-root-secret
&lt;/code>&lt;/pre>&lt;p>And now, I can use &lt;code>selfsigned-issuer&lt;/code> to issue TLS certificates for the ingress resources (&lt;em>Traefik ingress in the k3s&amp;rsquo;s case&lt;/em>). E.g. to play around, I run an open-source version of &lt;a href="https://languagetool.org/dev">LanguageTool&lt;/a> server. The ingress manifests for the server looks like as following:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
 name: languagetool-server
 annotations:
 kubernetes.io/ingress.class: traefik
 cert-manager.io/issuer: selfsigned-issuer
spec:
 rules:
 - host: languagetool.k8s.pi.home
 http:
 paths:
 - path: /
 pathType: ImplementationSpecific
 backend:
 service:
 name: languagetool-server
 port:
 number: 8010
 tls:
 - hosts: [languagetool.k8s.pi.home]
 secretName: languagetool-server-cert
&lt;/code>&lt;/pre>&lt;p>Of course, any certificate signed by my CA won&amp;rsquo;t be automatically trusted by anyone, including my own system. If I try to access &lt;code>https://languagetool.k8s.pi.home&lt;/code>, any HTTP client will raise a &amp;ldquo;failed to verify the legitimacy of the server&amp;rdquo; issue. I don&amp;rsquo;t know if there is a better way to solve that, but I can hack that around by installing the cluster&amp;rsquo;s root CA certificate into the system&amp;rsquo;s keychain, and telling the system, that it should &amp;ldquo;trust&amp;rdquo; the certificate:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl get secret/selfsigned-ca-root-secret -o json \
 | jq -r &amp;#39;.data[&amp;#34;ca.crt&amp;#34;]&amp;#39; \
 | base64 -D &amp;gt; ~/tmp/selfsigned-root-ca.crt
$ open ~/tmp/selfsigned-root-ca.crt
&lt;/code>&lt;/pre>&lt;p>Choose &amp;ldquo;Always trust&amp;rdquo; the certificate in the keychain&amp;rsquo;s certificate settings. The server looks &lt;em>legitimate&lt;/em> now!&lt;/p></description></item><item><title>Wireless-to-Ethernet island for homelab cluster: IPv6, NDP proxy and mDNS reflector</title><link>https://vladimir.varank.in/notes/2021/04/wireless-to-ethernet-island-for-homelab-cluster-ipv6-ndp-proxy-and-mdns-reflector/</link><pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate><guid>https://vladimir.varank.in/notes/2021/04/wireless-to-ethernet-island-for-homelab-cluster-ipv6-ndp-proxy-and-mdns-reflector/</guid><description>&lt;p>Initially, when I assembled &lt;a href="https://vladimir.varank.in/notes/2020/01/raspi-ubuntu-arm64-k3s/">a homelab cluster of Raspberry Pis&lt;/a>, everything was directly connected to my Wi-Fi router with the Ethernet cables. This worked fine but this &amp;ldquo;stack of boards&amp;rdquo; behind the sofa in the centre of our small flat bugged me a bit.&lt;/p>
&lt;p>Last year I decided to reorganise the cluster, turning it into a &lt;em>wireless-to-wired island&lt;/em>, which I could relocate anywhere within the flat, without doing any special cable management, while staying cheap and avoid stacking the appartment with even more gadgets. After going through a number of trials and errors, the final setup looks as the following:&lt;/p>
&lt;figure>&lt;img src="https://vladimir.varank.in/images/2021/homelab-pi-net-1.png"
 alt="Homelab cluster as a wireless-to-ethernet island (2021)" width="820">
&lt;/figure>

&lt;p>Different colours contour the connections between two logical subnets — more on that later. Here what we have on the schema (top to bottom):&lt;/p></description></item><item><title>Waveshare ESP8266 Driver Board Pins Mapping</title><link>https://vladimir.varank.in/notes/2020/07/waveshare-esp8266-driver-board-pins-mapping/</link><pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate><guid>https://vladimir.varank.in/notes/2020/07/waveshare-esp8266-driver-board-pins-mapping/</guid><description>&lt;p>I&amp;rsquo;ve been playing with an &lt;a href="https://www.waveshare.com/product/iot-communication/short-range-wireless/wifi/e-paper-esp8266-driver-board.htm">e-paper ESP866 Driver Board&lt;/a>, and a &lt;a href="https://www.waveshare.com/2.7inch-e-Paper.htm">2.7&amp;quot; E-Ink display&lt;/a> from Waveshare. Arduino C++ looks manageable. One strange thing, though. In both board’s documentation and &lt;a href="https://github.com/ZinggJM/GxEPD2">GxEPD2&lt;/a> library’s examples, they say the display is connected to pins as BUSY → GPIO16, RST → GPIO5, DC → GPIO4, CS → GPIO15. This mapping seems wrong.&lt;/p>
&lt;p>After digging through the code examples from &lt;a href="https://github.com/ZinggJM/GxEPD2">Waveshare’s Wiki&lt;/a>, the correct mapping is the following:&lt;/p>
&lt;p>BUSY → GPIO5,
RST → GPIO2,
DC → GPIO4,
CS → GPIO15&lt;/p>
&lt;p>That&amp;rsquo;s how the initialisation of the main &lt;code>GxEPD2&lt;/code> class for my 2.7&amp;quot; display looks like now:&lt;/p>
&lt;pre tabindex="0">&lt;code>#define ENABLE_GxEPD2_GFX 0
#include &amp;lt;GxEPD2_BW.h&amp;gt;

// mapping of Waveshare e-Paper ESP8266 Driver Board
GxEPD2_BW&amp;lt;GxEPD2_270, GxEPD2_270::HEIGHT&amp;gt; display(GxEPD2_270(/*CS=15*/ SS, /*DC=4*/ 4, /*RST=2*/ 2, /*BUSY=5*/ 5));
&lt;/code>&lt;/pre></description></item><item><title>k3s with Ubuntu Server (arm64) on Raspberry Pi 4</title><link>https://vladimir.varank.in/notes/2020/01/raspi-ubuntu-arm64-k3s/</link><pubDate>Sat, 11 Jan 2020 00:00:00 +0000</pubDate><guid>https://vladimir.varank.in/notes/2020/01/raspi-ubuntu-arm64-k3s/</guid><description>&lt;p>As I&amp;rsquo;ve &lt;a href="https://twitter.com/tvii/status/1215927299557797893?s=20">twitted&lt;/a> recently, I&amp;rsquo;m updating one of my Raspberry Pis to &lt;a href="https://ubuntu.com/download/raspberry-pi">Ubuntu Server 19.10 (arm64)&lt;/a>.&lt;/p>
&lt;h2 id="one-of-raspberry-pis">&amp;ldquo;One of Raspberry Pis&amp;rdquo;?&lt;/h2>
&lt;p>My home cluster is four &lt;a href="https://www.raspberrypi.org/products/raspberry-pi-4-model-b/?variant=raspberry-pi-4-model-b-2gb">Raspberry Pis 4 (2GB)&lt;/a>; all connected to my internet router through ethernet and
powered with &lt;a href="https://www.amazon.de/gp/product/B00PTLSH9G/ref=ppx_yo_dt_b_asin_title_o03_s00?ie=UTF8&amp;amp;psc=1">60W 6 USB-ports charger&lt;/a>. All Pis build a small Kubernetes cluster that runs with &lt;a href="https://k3s.io/">k3s&lt;/a>.&lt;/p>
&lt;p>All by one Pis run on &lt;a href="https://www.raspberrypi.org/downloads/raspbian/">Raspbian Buster Lite&lt;/a> and this setup&amp;rsquo;s been working pretty well until I&amp;rsquo;ve found out,
&lt;a href="https://www.aerospike.com/">Aerospike&lt;/a>, a database I required to run for a testing lab, only works on a 64-bit OS.&lt;/p>
&lt;p>Luckily, &lt;a href="https://ubuntu.com/download/raspberry-pi">Ubuntu Server has an arm64 version&lt;/a> built for Raspberry Pi. Thus, my working plan is to switch one Pi
to Ubuntu, compile and run a single-instance Aerospike server (&lt;em>and any other components, that require a 64-bit OS&lt;/em>) on this Pi, and provide a Kubernetes service in front of the DB, so other components in the cluster could access it as if it was
fully managed by Kubernetes.&lt;/p></description></item></channel></rss>