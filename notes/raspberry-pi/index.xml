<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>raspberry pi on Vladimir Varankin</title><link>https://vladimir.varank.in/notes/raspberry-pi/</link><description>Recent content in raspberry pi on Vladimir Varankin</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 10 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://vladimir.varank.in/notes/raspberry-pi/index.xml" rel="self" type="application/rss+xml"/><item><title>Self-signed certificates with k3s and cert-manager</title><link>https://vladimir.varank.in/notes/2021/07/self-signed-certificates-with-k3s-and-cert-manager/</link><pubDate>Sat, 10 Jul 2021 00:00:00 +0000</pubDate><guid>https://vladimir.varank.in/notes/2021/07/self-signed-certificates-with-k3s-and-cert-manager/</guid><description>At least for now, my homelab cluster (4x Raspberry Pi, k3s, etc) is available only for the devices on my local network, inside a custom DNS zone k8s.pi.home. I don&amp;rsquo;t think there are practical reasons to run anything with HTTPS in that setup, but there are cases, like browser extensions, where it&amp;rsquo;s required.
Turned out, in 2021, it&amp;rsquo;s fairly straight forward to set up a Certificate Authority (CA), that will issue TLS certificates to &amp;ldquo;secure&amp;rdquo; the ingress resources. At least, it&amp;rsquo;s way simpler comparing to how I remember it was back in the days. All thanks to cert-manager and some YAML.
First thing is to install cert-manager to the cluster. k3s comes with helm-controller, that gives us a way to manage helm charts with Custom Resource Definitions (CRD). The following manifest defines a new namespace, and a resource of a kind HelmChart, to install cert-manager inside this namespace:
apiVersion: v1 kind: Namespace metadata: name: cert-manager --- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: cert-manager namespace: kube-system spec: chart: cert-manager repo: https://charts.jetstack.io targetNamespace: cert-manager valuesContent: |- installCRDs: true prometheus: enabled: true servicemonitor: enabled: true After applying the manifest above — kubectl apply -f cert-manager.yml — define a self-signed certificate, which is used to bootstrap a CA:
apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: selfsigned-cluster-issuer spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: selfsigned-ca spec: isCA: true commonName: selfsigned-ca secretName: selfsigned-ca-root-secret privateKey: algorithm: ECDSA size: 256 issuerRef: name: selfsigned-cluster-issuer kind: ClusterIssuer group: cert-manager.io --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: selfsigned-issuer spec: ca: secretName: selfsigned-ca-root-secret And now, I can use selfsigned-issuer to issue TLS certificates for the ingress resources (Traefik ingress in the k3s&amp;rsquo;s case). E.g. to play around, I run an open-source version of LanguageTool server. The ingress manifests for the server looks like as following:
apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: languagetool-server annotations: kubernetes.io/ingress.class: traefik cert-manager.io/issuer: selfsigned-issuer spec: rules: - host: languagetool.k8s.pi.home http: paths: - path: / pathType: ImplementationSpecific backend: service: name: languagetool-server port: number: 8010 tls: - hosts: [languagetool.k8s.pi.home] secretName: languagetool-server-cert Of course, any certificate signed by my CA won&amp;rsquo;t be automatically trusted by anyone, including my own system. If I try to access https://languagetool.k8s.pi.home, any HTTP client will raise a &amp;ldquo;failed to verify the legitimacy of the server&amp;rdquo; issue. I don&amp;rsquo;t know if there is a better way to solve that, but I can hack that around by installing the cluster&amp;rsquo;s root CA certificate into the system&amp;rsquo;s keychain, and telling the system, that it should &amp;ldquo;trust&amp;rdquo; the certificate:
$ kubectl get secret/selfsigned-ca-root-secret -o json \ | jq -r &amp;#39;.data[&amp;#34;ca.crt&amp;#34;]&amp;#39; \ | base64 -D &amp;gt; ~/tmp/selfsigned-root-ca.crt $ open ~/tmp/selfsigned-root-ca.crt Choose &amp;ldquo;Always trust&amp;rdquo; the certificate in the keychain&amp;rsquo;s certificate settings. The server looks legitimate now!</description></item><item><title>Wireless-to-Ethernet island for homelab cluster: IPv6, NDP proxy and mDNS reflector</title><link>https://vladimir.varank.in/notes/2021/04/wireless-to-ethernet-island-for-homelab-cluster-ipv6-ndp-proxy-and-mdns-reflector/</link><pubDate>Mon, 26 Apr 2021 00:00:00 +0000</pubDate><guid>https://vladimir.varank.in/notes/2021/04/wireless-to-ethernet-island-for-homelab-cluster-ipv6-ndp-proxy-and-mdns-reflector/</guid><description>&lt;p>Initially, when I assembled &lt;a href="https://vladimir.varank.in/notes/2020/01/raspi-ubuntu-arm64-k3s/">a homelab cluster of Raspberry Pis&lt;/a>, everything was directly connected to my Wi-Fi router with the Ethernet cables. This worked fine but this &amp;ldquo;stack of boards&amp;rdquo; behind the sofa in the centre of our small flat bugged me a bit.&lt;/p>
&lt;p>Last year I decided to reorganise the cluster, turning it into a &lt;em>wireless-to-wired island&lt;/em>, which I could relocate anywhere within the flat, without doing any special cable management, while staying cheap and avoid stacking the appartment with even more gadgets. After going through a number of trials and errors, the final setup looks as the following:&lt;/p>
&lt;figure>&lt;img src="https://vladimir.varank.in/images/2021/homelab-pi-net-1.png"
 alt="Homelab cluster as a wireless-to-ethernet island (2021)" width="820"/>
&lt;/figure>

&lt;p>Different colours contour the connections between two logical subnets — more on that later. Here what we have on the schema (top to bottom):&lt;/p></description></item><item><title>Building Multi-Platform Docker Images with Travis CI and BuildKit</title><link>https://vladimir.varank.in/notes/2020/01/buildkit-multi-platform-travis-ci/</link><pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate><guid>https://vladimir.varank.in/notes/2020/01/buildkit-multi-platform-travis-ci/</guid><description>This is a lengthy note. If you don&amp;rsquo;t quite feel reading and only need the working example, go directly to the Travis CI build file.
The more I delve into the world of Raspberry Pi, the more I notice that &amp;ldquo;regular and boring&amp;rdquo; things on ARM are harder than I expected.
People build and distribute software exclusively for amd64. You read another &amp;ldquo;Kubernetes something&amp;rdquo; tutorial, that went viral on Twitter, and is fancy to try it out. Still, all helm charts, or whatever the author prefered, use Docker images built exclusively for amd64.
Docker toolchain has added the support for building multi-platform images in 19.x. However, it&amp;rsquo;s available only under the &amp;ldquo;experimental&amp;rdquo; mode. The topic of building multi-platform Docker images yet feels underrepresented.
But first, what are multi-platform Docker images? When a client, e.g. Docker client, tries to pull an image, it must negotiate the details about what exactly to pull with the registry. The registry provides a manifest that describes the digest of the requested image, the volumes the image consists of, the platform this image can run on, etc. Optionally, the registry can provide a manifests list, which, as the name suggests, is a list of several manifests bundled into one. With the manifests list in hands, the client can figure out the particular digest of the image it needs to pull.
So multi-platform Docker images are just several images, whose manifests are bundled into the manifests list.
Imagine we want to pull the image golang:1.13.6-alpine3.10. Docker client will get the manifests list from Dockerhub. This list includes digests of several images, each built for the particular platform. If we&amp;rsquo;re on Raspberry Pi, running the current Raspbian Linux, which is arm/v7, the client will pick the corresponding image&amp;rsquo;s digest. Alternatively, we could choose to pull the image arm32v7/golang:1.13.6-alpine3.10 instead, and we ended up with the same image with the digest d72fa60fb5b9. Of course, to use a single universal image name, i.e. golang, on every platform is way more convenient.
You can read more about manifests in Docker registry documentation.
Does it mean I need to build different Docker images, for each platform I want to support? Well, yes. This is how, official images are built.
For every platform, the image is built and pushed to the registry under the name &amp;lt;platform&amp;gt;/&amp;lt;image&amp;gt;:&amp;lt;tag&amp;gt;, e.g. amd64/golang:1-alpine. And next, a manifests list, that combines all those platform-specific images, is built and pushed with the simple name &amp;lt;image&amp;gt;:&amp;lt;tag&amp;gt;.
Docker&amp;rsquo;s BuildKit provides a toolkit that, among other nice things, allows building multi-platform images on a single host. BuildKit is used inside Docker&amp;rsquo; buildx project, that is part of the recent Docker version.
One can use buildx, but, for this post, I wanted to try out, what would it look like to use BuildKit directly. For profefe, the system for continuous profiling of Go services, I set up Travis CI, that builds a multi-platform Docker image and pushes them to Dockerhub.
profefe is written in Go. That simplifies things, because, thanks to Go compiler, I don&amp;rsquo;t have to think about how to compile code for different platforms. The same Dockerfile will work fine on every platform.
Here&amp;rsquo;s how &amp;ldquo;deploy&amp;rdquo; stage of the build job looks like (see travis.yml on profefe&amp;rsquo;s GitHub).
dist: bionic language: go go: - 1.x jobs: include: - stage: deploy docker services: docker env: - PLATFORMS=&amp;#34;linux/amd64,linux/arm64,linux/arm/v7&amp;#34; install: - docker container run --rm --privileged multiarch/qemu-user-static --reset -p yes - docker container run -d --rm --name buildkitd --privileged moby/buildkit:latest - sudo docker container cp buildkitd:/usr/bin/buildctl /usr/local/bin/ - export BUILDKIT_HOST=&amp;#34;docker-container://buildkitd&amp;#34; script: skip deploy: - provider: script script: | buildctl build \ --progress=plain \ --frontend=dockerfile.v0 \ --local context=. --local dockerfile=. \ --opt filename=contrib/docker/Dockerfile \ --opt platform=$PLATFORMS \ --opt build-arg:VERSION=\&amp;#34;master\&amp;#34; \ --opt build-arg:GITSHA=\&amp;#34;$TRAVIS_COMMIT\&amp;#34; \ --output type=image,\&amp;#34;name=profefe/profefe:git-master\&amp;#34;,push=true on: repo: profefe/profefe branch: master before_deploy: - echo &amp;#34;$DOCKER_PASSWORD&amp;#34; | docker login --username &amp;#34;$DOCKER_USERNAME&amp;#34; --password-stdin after_failure: - buildctl debug workers ls - docker container logs buildkitd It&amp;rsquo;s a lot happening here, but I&amp;rsquo;ll describe the most critical parts.
Let&amp;rsquo;s start with dist: bionic.
We run the builds under Ubuntu 18.04 (Bionic Beaver). To be able to build multi-platform images on a single amd64 host, BuildKit uses QEMU to emulate other platforms. That requires Linux kernel 4.8, so even Ubuntu 16.04 (Xenial Xerus) should work.
The top-level details on how the emulation works are very well described in https://www.kernel.org/doc/html/latest/admin-guide/binfmt-misc.html
In short, we tell the component of the kernel (binfmt_misc) to use QEMU when the system executes a binaries built for a different platform. The following call in the &amp;ldquo;install&amp;rdquo; step is what&amp;rsquo;s doing that:
- docker container run --rm --privileged multiarch/qemu-user-static --reset -p yes Under the hood, the container runs a shell script from QEMU project, that registers the emulator as an executor of binaries from the external platforms.
If you think, that running a docker container to do the manipulations with the host&amp;rsquo;s OS looks weird, well&amp;hellip; I can&amp;rsquo;t agree more. Probably, a better approach would be to install qemu-user-static, which would do the proper setup. Unfortunately, the current package&amp;rsquo;s version for Ubuntu Bionic doesn&amp;rsquo;t do the registration as we need it. I.e. its post-install doesn&amp;rsquo;t add the &amp;quot;F&amp;quot; flag (&amp;ldquo;fix binaries&amp;rdquo;), which is crucial for our goal. Let&amp;rsquo;s just agree,that docker-run will do ok for the demonstrational purpose.
- docker container run -d --rm --name buildkitd --privileged moby/buildkit:latest - sudo docker container cp buildkitd:/usr/bin/buildctl /usr/local/bin/ - export BUILDKIT_HOST=&amp;#34;docker-container://buildkitd&amp;#34; This is another &amp;ldquo;docker-run&amp;rsquo;ism&amp;rdquo;. We start BuildKit&amp;rsquo;s buildkitd daemon inside the container, attaching it to the Docker daemon that runs on the host (&amp;ldquo;privileged&amp;rdquo; mode). Next, we copy buildctl binary from the container to the host system and set BUILDKIT_HOST environment variable, so buildctl knew where its daemon runs.
Alternatively, we could install BuildKit from GitHub and run the daemon directly on the build host. YOLO.
before_deploy: - echo &amp;#34;$DOCKER_PASSWORD&amp;#34; | docker login --username &amp;#34;$DOCKER_USERNAME&amp;#34; --password-stdin To be able to push the images to the registry, we need to log in providing Docker credentials to host&amp;rsquo;s Docker daemon. The credentials are set as Travis CI&amp;rsquo;s encrypted environment variables ([refer to Travis CI docs])](https://docs.travis-ci.com/user/environment-variables/)).
buildctl build \ --progress=plain \ --frontend=dockerfile.v0 \ --local context=. --local dockerfile=. \ --opt filename=contrib/docker/Dockerfile \ --opt platform=$PLATFORMS \ --opt build-arg:VERSION=\&amp;#34;master\&amp;#34; \ --opt build-arg:GITSHA=\&amp;#34;$TRAVIS_COMMIT\&amp;#34; \ --output type=image,\&amp;#34;name=profefe/profefe:git-master\&amp;#34;,push=true This is the black box where everything happens. Magically!
We run buildctl stating that it must use the specified Dockerfile; it must build the images for defined platforms (I specified linux/amd64,linux/arm64,linux/arm/v7), create a manifests list tagged as the desired image (profefe/profefe:&amp;lt;version&amp;gt;), and push all the images to the registry.
buildctl debug workers ls shows what platforms does BuildKit on this host support. I listed only those I&amp;rsquo;m currently intrested with.
And that&amp;rsquo;s all. This setup automatically builds and pushes multi-platform Docker images for profefe (https://hub.docker.com/p/profefe/profefe) on a commit to project&amp;rsquo;s &amp;ldquo;master&amp;rdquo; branch on GitHub.
As I hope you&amp;rsquo;ve seen, support for multi-platform is getting easier and things that were hard a year ago are only mildly annoying now :)
If you have any comments or suggestions, reach out to me on Twitter or discuss this note on r/docker Reddit.
Some more reading on the topic:
Documentation for BuildKit project Building multi-platform images with docker and buildx Docker Official Images are now Multi-platform, Docker official announcement</description></item><item><title>k3s with Ubuntu Server (arm64) on Raspberry Pi 4</title><link>https://vladimir.varank.in/notes/2020/01/raspi-ubuntu-arm64-k3s/</link><pubDate>Sat, 11 Jan 2020 00:00:00 +0000</pubDate><guid>https://vladimir.varank.in/notes/2020/01/raspi-ubuntu-arm64-k3s/</guid><description>&lt;p>As I&amp;rsquo;ve &lt;a href="https://twitter.com/tvii/status/1215927299557797893?s=20">twitted&lt;/a> recently, I&amp;rsquo;m updating one of my Raspberry Pis to &lt;a href="https://ubuntu.com/download/raspberry-pi">Ubuntu Server 19.10 (arm64)&lt;/a>.&lt;/p>
&lt;h2 id="one-of-raspberry-pis">&amp;ldquo;One of Raspberry Pis&amp;rdquo;?&lt;/h2>
&lt;p>My home cluster is four &lt;a href="https://www.raspberrypi.org/products/raspberry-pi-4-model-b/?variant=raspberry-pi-4-model-b-2gb">Raspberry Pis 4 (2GB)&lt;/a>; all connected to my internet router through ethernet and
powered with &lt;a href="https://www.amazon.de/gp/product/B00PTLSH9G/ref=ppx_yo_dt_b_asin_title_o03_s00?ie=UTF8&amp;amp;psc=1">60W 6 USB-ports charger&lt;/a>. All Pis build a small Kubernetes cluster that runs with &lt;a href="https://k3s.io/">k3s&lt;/a>.&lt;/p>
&lt;p>All by one Pis run on &lt;a href="https://www.raspberrypi.org/downloads/raspbian/">Raspbian Buster Lite&lt;/a> and this setup&amp;rsquo;s been working pretty well until I&amp;rsquo;ve found out,
&lt;a href="https://www.aerospike.com/">Aerospike&lt;/a>, a database I required to run for a testing lab, only works on a 64-bit OS.&lt;/p>
&lt;p>Luckily, &lt;a href="https://ubuntu.com/download/raspberry-pi">Ubuntu Server has an arm64 version&lt;/a> built for Raspberry Pi. Thus, my working plan is to switch one Pi
to Ubuntu, compile and run a single-instance Aerospike server (&lt;em>and any other components, that require a 64-bit OS&lt;/em>) on this Pi, and provide a Kubernetes service in front of the DB, so other components in the cluster could access it as if it was
fully managed by Kubernetes.&lt;/p></description></item></channel></rss>