<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=color-scheme content="light dark">
<meta http-equiv=x-ua-compatible content="ie=edge">
<title>Notes - Vladimir Varankin</title>
<link rel=canonical href=https://vladimir.varank.in/notes/>
<link rel=alternate type=application/rss+xml href=https://vladimir.varank.in/notes/index.xml title="Notes from Vladimir Varankin">
<meta property="og:title" content="Notes">
<meta property="og:description" content>
<meta property="og:type" content="website">
<meta property="og:url" content="https://vladimir.varank.in/notes/">
<style>:root{--background-color:#fff;--text-color:rgb(19, 19, 19);--text-underline-color:rgba(19, 19, 19, 0.5);--header-color:#111;--hr-color:rgb(143, 150, 163);--link-color:rgb(0, 16, 161);--link-underline-color:rgba(0, 16, 161, 0.3);--link-active-color:rgb(231, 19, 19);--link-active-underline-color:rgba(231, 19, 19, 0.7);--tag-color:rgb(85, 63, 63);--tag-underline-color:rgba(85, 63, 63, 0.5)}html{font-size:16px;height:100%;margin:0;padding:0}body{background:var(--background-color);color:var(--text-color);display:grid;grid-template-rows:auto 1fr auto;font-family:Charter,Georgia,serif;font-size:1.1rem;line-height:1.3;margin:5px 1%;margin:5px max(7px,1%);margin:5px clamp(7px,1.5vw,14px);min-height:calc(100vh - 10px)}h1,h2,h3,h4,h5,h6{color:var(--header-color);font-family:Athelas,Georgia,serif;line-height:1;margin:1.5em 0 .7em}h1{font-size:40px;margin:.5em 0 .2em}a{color:#0010a1;color:var(--link-color);text-decoration-color:var(--link-underline-color);text-decoration-thickness:.04em;text-underline-offset:.1em}a:hover,a:active{color:#be0000;color:var(--link-active-color);text-decoration-color:var(--link-active-underline-color)}ul,ol{box-sizing:border-box;max-width:760px;padding:0}ol{padding-left:1.2em}ul>li{list-style:none;margin:.5em 0}figure{margin:0;margin-block-start:1em;margin-block-end:1em;padding:0}figcaption{font-size:16px;font-style:italic}pre{line-height:1.1;margin:2em 1em}code{font-family:sf mono,Menlo,Consolas,monospace;font-size:16px}pre code{font-size:15px;tab-size:4}hr{color:var(--hr-color);border:none;border-bottom:1px solid;margin:.5em 0;width:76%}.site-nav{display:flex;flex-wrap:wrap;letter-spacing:.3em;padding-bottom:16px}.site-nav a,.site-nav span{margin-right:.6em;letter-spacing:0}.site-nav-item-active{color:var(--link-active-color);text-decoration-color:var(--link-active-underline-color)}.site-content{font-size:19px;max-width:940px}.site-content img{max-width:100%;max-width:calc(100vw - 15px)}.site-content pre{max-width:calc(100vw - 2em);overflow-x:visible}.site-footer{font-size:14px;margin:50px 0 0}.site-footer a{color:var(--text-color);text-decoration-color:inherit;text-decoration-color:var(--text-underline-color)}.main-head{display:flex;align-items:baseline;gap:8px}.main-head-meta{font-size:16px;margin:0;padding:0}.main-head-meta a{color:var(--text-color);text-decoration-color:inherit;text-decoration-color:var(--text-underline-color);margin:0 5px 0 0}.main-head-meta a:hover,.main-head-meta a:active{color:var(--link-active-color);text-decoration-color:var(--link-active-underline-color)}.article{margin:0 0 50px}.article:last-child{margin-bottom:0}.section-head{font-size:28px}.section-head h1,.section-head h2{font-size:1em;margin-top:1em}.section-head h2{margin-bottom:0}.snippet{margin:0 0 40px}.snippet:last-of-type{margin:0}.snippet p{margin:0 0 .3em}.snippet-head h2{font-size:21px;margin:1em 0 .2em}.meta{font-size:14px;line-height:1;padding:5px 0 0}.tag{color:var(--tag-color);text-decoration-color:var(--tag-underline-color);margin:0 0 0 7px;text-transform:lowercase}.meta-topics-list{line-height:1.4;padding:34px 0 0}.meta-topics-list .tag{display:inline-block;margin:0 1ex 0 0}.glitch:before{background:var(--background-color);position:absolute;content:attr(data-text);clip:rect(0,900px,0,0);animation:glitch-anim 2s infinite linear alternate-reverse;margin-left:-5px;text-shadow:1px 0 red}@media(prefers-reduced-motion){.glitch:before{display:none}}@keyframes glitch-anim{0%{clip:rect(3px,9999px,93px,0)}5%{clip:rect(53px,9999px,78px,0)}10%{clip:rect(10px,9999px,75px,0)}15%{clip:rect(32px,9999px,40px,0)}20%{clip:rect(65px,9999px,62px,0)}25%{clip:rect(31px,9999px,14px,0)}30%{clip:rect(94px,9999px,87px,0)}35%{clip:rect(81px,9999px,41px,0)}40%{clip:rect(45px,9999px,50px,0)}45%{clip:rect(82px,9999px,41px,0)}50%{clip:rect(71px,9999px,3px,0)}55%{clip:rect(75px,9999px,60px,0)}60%{clip:rect(20px,9999px,49px,0)}65%{clip:rect(67px,9999px,92px,0)}70%{clip:rect(47px,9999px,55px,0)}75%{clip:rect(63px,9999px,90px,0)}80%{clip:rect(70px,9999px,92px,0)}85%{clip:rect(41px,9999px,60px,0)}90%{clip:rect(56px,9999px,79px,0)}95%{clip:rect(21px,9999px,68px,0)}100%{clip:rect(15px,9999px,72px,0)}}</style>
</head>
<body>
<nav class=site-nav>
<a href=/ class=glitch data-text="Vladimir Varankin">Vladimir Varankin</a>
<a href=/notes/ class=site-nav-item-active>Notes</a>
<a href=/publications/>Publications</a>
<span> · </span>
<a href=https://github.com/narqo>GitHub</a>
<a href=https://twitter.com/tvii>Twitter</a>
<a href=https://t.me/varankinv>Telegram</a>
<a href=https://keybase.io/varankinv>Keybase</a>
</nav>
<main class=site-content>
<header class=main-head>
<h1>Notes</h1>
/ <menu class=main-head-meta>
<a href=/notes/berlin/>Berlin</a>
<a href=/notes/go/>Go</a>
<a href=/notes/homelab/>Homelab</a>
<a href=/notes/kubernetes/>Kubernetes</a>
<a href=/notes/all/>Everything</a>
</menu>
</header>
<article class=article data-weight=0 data-word-count=100 data-reading-time=1>
<header class=section-head>
<h2><a href=/notes/2022/02/love-hate-material/>Love. Hate. Material Design</a></h2>
</header>
<p>If Chrome had a notch</p>
<figure><img src=/images/2022/material-chrome-cut-1x.jpg alt="Chrome notch" width=1000>
</figure>
<p><a href=/notes/2022/02/love-hate-material/>Another witty picture under the cut…</a></p>
<footer class=meta>
<time datetime=2022-02-02T12:00:00Z>February 2, 2022</time><a class=tag href=/notes/material-design/>material design</a><a class=tag href=/notes/random/>random</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=1200 data-reading-time=6>
<header class=section-head>
<h2><a href=/notes/2022/01/i-got-covid/>I've got COVID. What do I do next?</a></h2>
</header>
<p>Go to <a href=https://www.berlin.de/corona/massnahmen/abstands-und-hygieneregeln/>https://www.berlin.de/corona/massnahmen/abstands-und-hygieneregeln/</a> for the information (in German) about the latest regulations in Berlin. For the up-to-date information in your region, consult with your local authorities.</p>
<p><em>Everything I list below are the steps I found relevant, after I&rsquo;ve self-tested positive for COVID in Berlin, in the end of January 2022</em>.</p>
<h3 id=friday-28th-january-2022>Friday, 28th January 2022</h3>
<p>I woke up with some <em>mild</em> symptoms of cold. On the day before I worked from home, and only had a usual an hour-long walk around Prenzlauer Berg — Mitte after the workday. The rapid antigen test (<em>Schnelltest</em>) was negative.</p>
<p>While still working from home, I took a &ldquo;quiet Friday&rdquo; at work, to simply focused on some mundane routines.</p>
<p>I went for a short walk in the afternoon, bought some grocery, and headed home.</p>
<h3 id=saturday-29th-january>Saturday, 29th January</h3>
<p>Didn&rsquo;t feel anywhere better or worse: mostly had a runny nose and a dry throat. I didn&rsquo;t want to wait in the line for a quick-test, so I&rsquo;ve just walked around the city for an hour and went home.</p>
<h3 id=sunday-30th-january>Sunday, 30th January</h3>
<p>It felt the same as it&rsquo;d been the day before, although the dry throat started to feel a bit more intense. Walked around for an hour, came home, made some coffee ;)</p>
<p>A couple hours later I started to cough. I made a self-test — we still had some at home — and,</p>
<blockquote>
<p><em>Here we are! Welcome to the &ldquo;two-stripes&rdquo; club</em>.</p>
</blockquote>
<p><a href=/notes/2022/01/i-got-covid/>Keep reading… </a></p>
<footer class=meta>
<time datetime=2022-01-31T12:00:00Z>January 31, 2022</time><a class=tag href=/notes/berlin/>berlin</a><a class=tag href=/notes/covid-19/>COVID-19</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=200 data-reading-time=1>
<header class=section-head>
<h2><a href=/notes/2021/12/error-messages-in-go/>Error messages in Go</a></h2>
</header>
<p>When Go code propagates an error, the following pattern is very popular:</p>
<pre tabindex=0><code>fmt.Errorf(&quot;failed to find a parking slot: %w&quot;, err)
// Or
fmt.Errorf(&quot;could not call mom: %w&quot;, err)
</code></pre><p>These &ldquo;could not&rdquo;, &ldquo;failed to&rdquo;, &ldquo;unable to&rdquo; make sense when my mind is in the local context of the function, method or package. But, in most of the cases I have to deal with, it makes the resulting log message overloaded with informational garbage:</p>
<pre tabindex=0><code>unable to ask about the cat: failed to call mom: failed to do request: Get https://: context canceled
</code></pre><p>While discussing this issue with a colleague, we came up with the following &ldquo;better&rdquo; strategy:</p>
<ol>
<li>only the logger should express its attitude to the facts, using words &ldquo;error&rdquo;, &ldquo;failed&rdquo;, etc</li>
<li>the business code must operate only with facts, e.g. &ldquo;call mom&rdquo;.</li>
</ol>
<pre tabindex=0><code>err := CallMom(number)
if err != nil {
    return fmt.Errorf(&quot;call mom (tel %s): %w&quot;, number, err)
}
</code></pre><p>This renders as following to the application logs:</p>
<pre tabindex=0><code>error: ask about cat: call mom (tel 123): make request: Get https://: context canceled
</code></pre><p>For a long stack of errors, this makes the full error message more dense, showing more useful information per line.</p>
<footer class=meta>
<time datetime=2021-12-30T12:00:00Z>December 30, 2021</time><a class=tag href=/notes/go/>Go</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=300 data-reading-time=2>
<header class=section-head>
<h2><a href=/notes/2021/10/good-coffee-places-in-berlin/>Good coffee places in Berlin</a></h2>
</header>
<p>My definition of &ldquo;good coffee places&rdquo; includes, although, by no mean limited by, offering a &ldquo;not too fruity&rdquo; (sometimes &ldquo;chocolaty&rdquo;) Filter coffee or a decent Americano.</p>
<p><a href=/notes/2021/10/good-coffee-places-in-berlin/>Keep reading… </a></p>
<footer class=meta>
<time datetime=2021-10-10T12:00:00Z>October 10, 2021</time><a class=tag href=/notes/berlin/>berlin</a><a class=tag href=/notes/coffee/>coffee</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=800 data-reading-time=4>
<header class=section-head>
<h2><a href=/notes/2021/09/making-sense-of-requests-for-cpu-resources-in-kubernetes/>Making sense of requests for CPU resources in Kubernetes</a></h2>
</header>
<p>Kubernetes allows a container to request several resource types:</p>
<pre tabindex=0><code>apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  containers:
  - name: my-app
    image: images.example/my-app
    resources:
      requests:
        cpu: &quot;100m&quot;
        memory: &quot;64Mi&quot;
      limits:
        cpu: &quot;500m&quot;
        memory: &quot;128Mi&quot;
</code></pre><p>One particularly confusing type of the resource for me was <code>cpu</code>. For example, in the manifest above, the <code>my-app</code> container declares a request for &ldquo;100m&rdquo; of the CPU. What does that mean?</p>
<p>Kubernetes&rsquo;s documentation on <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/>managing containers resources</a> describes that as following:</p>
<blockquote>
<p>Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors.
CPU is always requested as an absolute quantity, never as a relative quantity; 0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine.</p>
</blockquote>
<p>A misleading (but fairly common) mental modal amongst the developers, is that the application inside the Pod&rsquo;s container is &ldquo;boxed&rdquo; by the container runtime, as if the application received a dedicated slice of the machine&rsquo;s resources. That&rsquo;s not exactly how it works. At least, it&rsquo;s not if Kubernetes uses Docker as the underlying container runtime.</p>
<p>I find it easier to think about the requested resources as a way for an application to &ldquo;hint&rdquo; to Kubernetes scheduler about how much resources the application <em>thinks</em> it needs.</p>
<p>Kubernetes scheduler keeps the &ldquo;accounting&rdquo; for how much of total resources the tenants of the cluster requested, and how much the cluster&rsquo;s machines have to offer. It&rsquo;s important to stress that the &ldquo;accounting&rdquo; is done only base on the requests for the resources. That is, the scheduler doesn&rsquo;t check if the container uses the resources it requested. If a container requested more than half of the node&rsquo;s CPU resources, e.g. &ldquo;1100m&rdquo; on a 2 vCPU node, Kubernetes won&rsquo;t deploy more than a single replica of this container to that node, nomatter if the application inside the container is idle.</p>
<blockquote>
<p>Each node has a maximum capacity for each of the resource types: the amount of CPU and memory it can provide for Pods. The scheduler ensures that, for each resource type, the sum of the resource requests of the scheduled containers is less than the capacity of the node. Note that although actual memory or CPU resource usage on nodes is very low, the scheduler still refuses to place a Pod on a node if the capacity check fails.</p>
</blockquote>
<p>What has been requested, has been booked.</p>
<p>Of course, &ldquo;accounting&rdquo; of the existing resources is only half of the story. In the same documentation about <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/>managing containers resources</a>, they talk about the implementation details of what happens after a container gets scheduled to a node, refering to Docker&rsquo;s documentation about <a href=https://docs.docker.com/engine/reference/run/#cpu-share-constraint>CPU share constraint</a>. This details for how Docker itself juggles the CPU shares between the running containers can be even more confusing but the interesting part to remember is the following:</p>
<blockquote>
<p>The proportion [of CPU cycles] will only apply when CPU-intensive processes are running. When tasks in one container are idle, other containers can use the left-over CPU time. [..]
On a multi-core system, the shares of CPU time are distributed over all CPU cores. Even if a container is limited to less than 100% of CPU time, it can use 100% of each individual CPU core.</p>
</blockquote>
<p>The important difference between the ideas of a &ldquo;box&rdquo; and a &ldquo;hint&rdquo; is that the &ldquo;hint&rdquo; doesn&rsquo;t prevent the application from consuming the whole node&rsquo;s CPU resources, when it requested only half of it — containers are not the VMs, afterall. Using the same example of a cluster&rsquo;s node with 2 vCPU, a multi-threaded application inside a container still sees two CPU cores. If there are no other tenants on the node, there is no one who the application has to compete for its share of CPU resources.</p>
<p>As mentioned earlier, this applies to Docker container runtime. Don&rsquo;t forget to consult with the documentation provided by the particular Kubernetes distribution when you make the decisions, that includes the fine-tunning of cluster resources. Docker is still the runtime AWS EKS uses for <a href=https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html>Kubernetes up to version 1.21</a>. But things might work differently for your cluster as Kubernetes providers switch to alternative container runtimes.</p>
<p><em>Let&rsquo;s discuss this note on <a href="https://news.ycombinator.com/item?id=28668859">Hacker News</a> and <a href=https://twitter.com/tvii/status/1442427866961088513>Twitter</a>.</em></p>
<footer class=meta>
<time datetime=2021-09-26T12:00:00Z>September 26, 2021</time><a class=tag href=/notes/kubernetes/>kubernetes</a><a class=tag href=/notes/docker/>docker</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=400 data-reading-time=2>
<header class=section-head>
<h2><a href=/notes/2021/08/one-step-closer-to-tabless-workflow/>One step closer to "Tabless" workflow</a></h2>
</header>
<p>Many years ago I embraced &ldquo;tabless&rdquo; development workflow: I use buffers, when I&rsquo;m in Vim; I also switch tabs off in both Goland and VS Code, as the first thing after I install the IDEs to a new laptop.</p>
<p>I&rsquo;m trying the same with Firefox web-browser now:</p>
<figure><img src=/images/2021/firefox-91-2021-a.jpg alt="Tabless Firefox 91" width=1100>
</figure>
<ol>
<li>
<p>Enable <code>toolkit.legacyUserProfileCustomizations.stylesheets</code> switch in Firefox&rsquo;s config (via <code>about:config</code> page or inside <code>user.js</code>).</p>
</li>
<li>
<p>Place the styles below into <code>%PROFILE%/chrome/userChrome.css</code> (I pick the actual path to the profile directory from Firefox&rsquo;s <code>about:support</code> page).</p>
</li>
</ol>
<pre tabindex=0><code>// Hide the tabs.
// Beware that hidding the tabs with &quot;display: none&quot; will ruine you browser's recent history,
// I've learned that in a hard way ;)
#TabsToolbar &gt; .toolbar-items {
    opacity: 0;
    pointer-events: none;
}

// Pull the navigation bar up, on top of the empty space, that left after we'd hidden the tabs.
#nav-bar {
    margin-top: calc((7px + var(--tab-min-height)) * -1);
}

// On macOS, when not in full screen, shift the urlbar's panel to the right,
// after close-minimise-expand buttons.
:root:not([inFullscreen]) #nav-bar-customization-target {
    margin-left: 65px;
}
</code></pre><p>I looked at <a href=https://searchfox.org/mozilla-central/source/browser/base/content>mozilla-central/··/navigator-toolbox.inc.xhtml</a> to get the structure of Firefox&rsquo;s UI.</p>
<ol start=3>
<li>
<p>Pick some nice and clean theme. My kudos to <a href=https://addons.mozilla.org/en-GB/firefox/addon/safari-macos-monterey-light/>Safari - MacOS Monterey Light</a> by a person nicknamed <a href=https://addons.mozilla.org/en-GB/firefox/user/16761237/>notcat</a>.</p>
</li>
<li>
<p>(<em>Optional</em>) In Firefox&rsquo;s &ldquo;General&rdquo; preferences, switch off &ldquo;Ctrl+Tab circles through tabs&rdquo;. With that, pressing Ctrl+Tab exposes all currently open pages (similar to how on macOS or other OS one switches between the opened applications with ⌘+Tab).</p>
</li>
</ol>
<hr>
<p>Overall I&rsquo;m fairly happy with how it ended up. Although, some things aren&rsquo;t quite ideal yet (might add more as I use this setup):</p>
<p>I wish there was a shortkey to enter a &ldquo;modal mode&rdquo;, where I could filter the list of open pages, to search for a particular page, and to switch to this page. Something similar to <code>:ls</code> command in Vim, or ⌘+E in Goland. I can use Firefox&rsquo;s &ldquo;Search amongst Tabs&rdquo; for that (press ⌘+L to focus into the URL bar, and querying with &ldquo;%[space]") but that requires some getting used to.</p>
<p>Firefox has &ldquo;Show all tabs&rdquo; button (Ctrl+Shift+Tab) but the way it works, at least in Firefox 91, is very confusing and random. It seems to me, its behaviour is tightly coupled to the browser&rsquo;s Tab UI.</p>
<footer class=meta>
<time datetime=2021-08-23T12:00:00Z>August 23, 2021</time><a class=tag href=/notes/design/>design</a><a class=tag href=/notes/firefox/>firefox</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=500 data-reading-time=3>
<header class=section-head>
<h2><a href=/notes/2021/07/self-signed-certificates-with-k3s-and-cert-manager/>Self-signed certificates with k3s and cert-manager</a></h2>
</header>
<p>At least for now, my homelab cluster (<a href=/notes/2020/01/raspi-ubuntu-arm64-k3s/>4x Raspberry Pi, k3s, etc</a>) is available only for the devices <a href=/notes/2021/04/wireless-to-ethernet-island-for-homelab-cluster-ipv6-ndp-proxy-and-mdns-reflector/>on my local network</a>, inside a custom DNS zone <code>k8s.pi.home</code>. I don&rsquo;t think there are practical reasons to run anything with HTTPS in that setup, but there are cases, like browser extensions, where it&rsquo;s required.</p>
<p>Turned out, in 2021, it&rsquo;s fairly straight forward to set up a Certificate Authority (CA), that will issue TLS certificates to &ldquo;secure&rdquo; the ingress resources. At least, it&rsquo;s way simpler comparing to how I remember it was back in the days. All thanks to <a href=https://cert-manager.io>cert-manager</a> and some YAML.</p>
<p>First thing is to install cert-manager to the cluster. <a href=https://rancher.com/docs/k3s/latest/en/helm/>k3s comes with helm-controller</a>, that gives us a way to manage helm charts with Custom Resource Definitions (CRD). The following manifest defines a new namespace, and a resource of a kind <code>HelmChart</code>, to install cert-manager inside this namespace:</p>
<pre tabindex=0><code>apiVersion: v1
kind: Namespace
metadata:
  name: cert-manager

---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: cert-manager
  namespace: kube-system
spec:
  chart: cert-manager
  repo: https://charts.jetstack.io
  targetNamespace: cert-manager
  valuesContent: |-
    installCRDs: true
    prometheus:
      enabled: true
      servicemonitor:
        enabled: true
</code></pre><p>After applying the manifest above — <code>kubectl apply -f cert-manager.yml</code> — define a self-signed certificate, which is used to bootstrap a CA:</p>
<pre tabindex=0><code>apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-cluster-issuer
spec:
  selfSigned: {}

---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: selfsigned-ca
spec:
  isCA: true
  commonName: selfsigned-ca
  secretName: selfsigned-ca-root-secret
  privateKey:
    algorithm: ECDSA
    size: 256
  issuerRef:
    name: selfsigned-cluster-issuer
    kind: ClusterIssuer
    group: cert-manager.io

---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: selfsigned-issuer
spec:
  ca:
    secretName: selfsigned-ca-root-secret
</code></pre><p>And now, I can use <code>selfsigned-issuer</code> to issue TLS certificates for the ingress resources (<em>Traefik ingress in the k3s&rsquo;s case</em>). E.g. to play around, I run an open-source version of <a href=https://languagetool.org/dev>LanguageTool</a> server. The ingress manifests for the server looks like as following:</p>
<pre tabindex=0><code>apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: languagetool-server
  annotations:
    kubernetes.io/ingress.class: traefik
    cert-manager.io/issuer: selfsigned-issuer
spec:
  rules:
  - host: languagetool.k8s.pi.home
    http:
      paths:
      - path: /
        pathType: ImplementationSpecific
        backend:
          service:
            name: languagetool-server
            port:
              number: 8010
  tls:
  - hosts: [languagetool.k8s.pi.home]
    secretName: languagetool-server-cert
</code></pre><p>Of course, any certificate signed by my CA won&rsquo;t be automatically trusted by anyone, including my own system. If I try to access <code>https://languagetool.k8s.pi.home</code>, any HTTP client will raise a &ldquo;failed to verify the legitimacy of the server&rdquo; issue. I don&rsquo;t know if there is a better way to solve that, but I can hack that around by installing the cluster&rsquo;s root CA certificate into the system&rsquo;s keychain, and telling the system, that it should &ldquo;trust&rdquo; the certificate:</p>
<pre tabindex=0><code>$ kubectl get secret/selfsigned-ca-root-secret -o json \
  | jq -r '.data[&quot;ca.crt&quot;]' \
  | base64 -D &gt; ~/tmp/selfsigned-root-ca.crt
$ open ~/tmp/selfsigned-root-ca.crt
</code></pre><p>Choose &ldquo;Always trust&rdquo; the certificate in the keychain&rsquo;s certificate settings. The server looks <em>legitimate</em> now!</p>
<footer class=meta>
<time datetime=2021-07-10T12:00:00Z>July 10, 2021</time><a class=tag href=/notes/homelab/>homelab</a><a class=tag href=/notes/raspberry-pi/>raspberry pi</a><a class=tag href=/notes/k3s/>k3s</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=1600 data-reading-time=8>
<header class=section-head>
<h2><a href=/notes/2021/04/wireless-to-ethernet-island-for-homelab-cluster-ipv6-ndp-proxy-and-mdns-reflector/>Wireless-to-Ethernet island for homelab cluster: IPv6, NDP proxy and mDNS reflector</a></h2>
</header>
<p>Initially, when I assembled <a href=/notes/2020/01/raspi-ubuntu-arm64-k3s/>a homelab cluster of Raspberry Pis</a>, everything was directly connected to my Wi-Fi router with the Ethernet cables. This worked fine but this &ldquo;stack of boards&rdquo; behind the sofa in the centre of our small flat bugged me a bit.</p>
<p>Last year I decided to reorganise the cluster, turning it into a <em>wireless-to-wired island</em>, which I could relocate anywhere within the flat, without doing any special cable management, while staying cheap and avoid stacking the appartment with even more gadgets. After going through a number of trials and errors, the final setup looks as the following:</p>
<figure><img src=/images/2021/homelab-pi-net-1.png alt="Homelab cluster as a wireless-to-ethernet island (2021)" width=820>
</figure>
<p>Different colours contour the connections between two logical subnets — more on that later. Here what we have on the schema (top to bottom):</p>
<p><a href=/notes/2021/04/wireless-to-ethernet-island-for-homelab-cluster-ipv6-ndp-proxy-and-mdns-reflector/>Keep reading… </a></p>
<footer class=meta>
<time datetime=2021-04-26T12:00:00Z>April 26, 2021</time><a class=tag href=/notes/homelab/>homelab</a><a class=tag href=/notes/raspberry-pi/>raspberry pi</a><a class=tag href=/notes/ipv6/>IPv6</a><a class=tag href=/notes/mdns/>mDNS</a><a class=tag href=/notes/ndppd/>ndppd</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=100 data-reading-time=1>
<header class=section-head>
<h2><a href=/notes/2021/04/development-environment-in-2020/>Development environment in 2020</a></h2>
</header>
<figure><img src=/images/2021/state-of-dev-2020-1.png alt="Development environment in 2020" width=600>
</figure>
<p><em>Last year I made this one, after involving myself into <a href=https://twitter.com/tvii/status/1343243854183604226>a very random discussion on Twitter</a>, about the unnecessary complexity and none-sense of modern days' development environments.</em></p>
<footer class=meta>
<time datetime=2021-04-05T12:00:00Z>April 5, 2021</time><a class=tag href=/notes/random/>random</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=700 data-reading-time=3>
<header class=section-head>
<h2><a href=/notes/2021/03/little-things-of-go-http-handlers/>Little things of Go HTTP handlers</a></h2>
</header>
<p>Every time I sketch an HTTP API in Go, I wrap the code of request handlers around these small but very convenient bits.</p>
<p>My handlers are methods or functions, that serve a request and, either write a (<em>positive</em>) response or return an error.</p>
<pre tabindex=0><code>// HandlerFunc is an HTTP handler function, that handles an HTTP request.
// It writes the response to http.ResponseWriter or returns an error.
type HandlerFunc func(w http.ResponseWriter, r *http.Request) error

// Handler is an adaptor for HandlerFunc, that converts the handler into http.Handler.
// It makes sure all errors returned from h are handled in a consistent manner.
func Handler(h HandlerFunc) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        err := h(w, r)
        if err != nil {
            handleError(w, r, err)
        }
    })
}
</code></pre><p>An error returned from <code>HandlerFunc</code> can be an indicator of a failure in request processing, <code>statusError</code>, or a general &ldquo;something didn&rsquo;t work&rdquo;-error. The later can contain the internal details, that the API must never expose to the user.</p>
<pre tabindex=0><code>// StatusError wraps an error err and contains the suggestion regarding to
// how the error should be communicated to the user.
//
// code must be a valid HTTP status code; text is the message to reply to user.
func StatusError(code int, text string, err error) error {
    return &amp;statusError{
        Code: code,
        Text: text,
        Err:  err,
    }
}

type statusError struct {
    Code int
    Text string
    Err  error
}

func (s statusError) Error() string {
    return s.Text
}

func (s statusError) Unwrap() error {
    return s.Err
}
</code></pre><p><code>handleError</code> is a helper function, which makes sure all errors returned from <code>HandlerFunc</code> are handled and replied to the user consistently. The internal details — the cause of the error — aren&rsquo;t exposed to the user, but the helper can provide a unified logging and metrics, which would be convenient when debugging the error later:</p>
<pre tabindex=0><code>var ErrNotFound = StatusError(http.StatusNotFound, &quot;Nothing found&quot;, nil)

func handleError(w http.ResponseWriter, r *http.Request, err error) {
    var statusErr *statusError
    if !errors.As(err, &amp;statusErr) {
        statusErr = &amp;statusError{Code: http.StatusInternalServerError, Text: &quot;Internal server error&quot;, Err: err}
    }

    rid := RequestIDFromContext(r.Context())
    resp := errResponse{
        RequestID: rid,
        Error:     statusErr.Text,
    }
    replyJSON(w, resp, statusErr.Code)

    // underlying error can be nil, as a special case, when the error is a client-side problem
    if err := errors.Unwrap(statusErr); err != nil {
        log.Errorw(&quot;request failed&quot;, &quot;request-id&quot;, rid, &quot;uri&quot;, r.RequestURI, &quot;error&quot;, err)
    }
}
</code></pre><p><code>replyJSON</code> is a helper function, which writes a JSON string to <code>http.ResponseWriter</code>, setting the proper HTTP headers.</p>
<pre tabindex=0><code>func replyJSON(w http.ResponseWriter, v interface{}, code int) {
    w.Header().Set(&quot;Content-Type&quot;, &quot;application/json&quot;)
    w.WriteHeader(code)
    err := json.NewEncoder(w).Encode(v)
    if err != nil {
        io.WriteString(w, `{&quot;code&quot;:500,&quot;error&quot;:`+strconv.Quote(err.Error())+`}`)
    }
}
</code></pre><p>How does it look in practice?</p>
<p>Below is an extract from a hypothetical API, with one single route <code>/api/login</code>, that takes an email, and replies with JSON, that contains this account&rsquo;s ID.</p>
<pre tabindex=0><code>func setupRoutes(mux *http.ServeMux) {
    authHandler := NewAuthHandler(···)
    mux.Handle(&quot;/api/login&quot;, Handler(authHandler.HandleLogin))
}

type AuthHandler {
    // internal dependencies
}

func (h *AuthHandler) HandleLogin(w http.ResponseWriter, r *http.Request) error {
    ctx := r.Context()

    req, err := DecodeLoginRequest(r)
    if err != nil {
        return fmt.Errorf(&quot;decode login request: %w&quot;, err)
    }
    if req.Email == &quot;&quot; {
        return StatusError(http.StatusBadRequest, &quot;email is required&quot;, nil)
    }

    accID, err := h.datastore.GetAccountByEmail(ctx, req.Email)
    if errors.Is(err, ErrNotExists) {
        return StatusError(http.StatusForbidden, &quot;account does not exist&quot;, err)
    }
    if err != nil {
        return fmt.Errorf(&quot;get account for %q: %w&quot;, req.Email, err)
    }

    resp := struct {
        ID string `json:&quot;id&quot;`
    }{
        ID: accID,
    }
    // ReplyJSON is a wrapper around internal replyJSON, that always responses with http.StatusOK
    ReplyJSON(w, resp)

    return nil
}
</code></pre><p><em>Do you have your own little things, that help you to lay out the boilerplate? Discuss this note on <a href=https://twitter.com/tvii/status/1377358308193935361>Twitter</a> or <a href=https://www.reddit.com/r/golang/comments/mhf04c/little_things_of_go_http_handlers/>Reddit</a>.</em></p>
<footer class=meta>
<time datetime=2021-03-31T12:00:00Z>March 31, 2021</time><a class=tag href=/notes/go/>Go</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=700 data-reading-time=3>
<header class=section-head>
<h2><a href=/notes/2021/01/you-dont-insert-unicode-null-character-as-postgres-jsonb/>(You don't) Insert unicode NULL character as Postgres jsonb</a></h2>
</header>
<p>With JSON data type it’s easy to treat Postgres as a document database, which doesn’t need strong schema. One can define a table, that has a field of a type <code>jsonb</code> and insert any valid JSON string (a “document”).</p>
<p>I’ve learned lately, that Postgres’s <code>jsonb</code> prohibits insertion of a valid JSON string if the string contains NULL (<code>U+0000</code>) character. Postgres’s own <a href=https://www.postgresql.org/docs/10/datatype-json.html>docs on JSON Types</a> says:</p>
<blockquote>
<p>RFC 7159 permits JSON strings to contain Unicode escape sequences denoted by \uXXXX. In the input function for the json type, Unicode escapes are allowed regardless of the database encoding, and are checked only for syntactic correctness. However, the input function for <code>jsonb</code> is stricter: it disallows Unicode escapes for non-ASCII characters (those above U+007F) unless the database encoding is UTF8. The <code>jsonb</code> type also rejects \u0000 (because that cannot be represented in PostgreSQL&rsquo;s text type), and it insists that any use of Unicode surrogate pairs to designate characters outside the Unicode Basic Multilingual Plane be correct.</p>
</blockquote>
<p>In my case, a Go backend inserts tracing logs to Postgres. A trace consists of multiple “spans”, some of which can contain the reply from an external API. As we found out, sometimes, in the event of a failure, the API replies with an empty GIF &lt;<em>facepalm/</em>>. Our backend converts the response to a string, marshals it to a JSON and later tries to insert the JSON into a Postgres table.</p>
<p>Consider the following Go code:</p>
<pre tabindex=0><code>// data is an empty GIF
var data = []byte{
    0x47, 0x49, 0x46, 0x38, 0x39, 0x61, 0x01, 0x00,
    0x01, 0x00, 0x80, 0x00, 0x00, 0x00, 0x00, 0x00,
    0xff, 0xff, 0xff, 0x21, 0xf9, 0x04, 0x01, 0x00,
    0x00, 0x00, 0x00, 0x2c, 0x00, 0x00, 0x00, 0x00,
    0x01, 0x00, 0x01, 0x00, 0x00, 0x02, 0x01, 0x44,
    0x00, 0x3b,
}

func main() {
    v, _ := json.Marshal(struct {
        Resp interface{} `json:&quot;resp,omitempty&quot;`
    }{
        Resp: string(data),
    })
    fmt.Printf(&quot;%s\n&quot;, v)
    // Output (truncated for readability):
    // {&quot;resp&quot;:&quot;GIF89a\u0001\u0000\u0001\u0000\ufffd\···\u0001D\u0000;&quot;}
}
</code></pre><p>Above, <code>json.Marshal</code> produces a perfectly valid JSON. But if I try to insert it into a Postgres table as <code>jsonb</code>, the insert fails with “unsupported Unicode escape sequence”:</p>
<pre tabindex=0><code>= CREATE TABLE logs (data jsonb);
= INSERT INTO logs VALUES ('{&quot;resp&quot;:&quot;GIF89a\u0001\u0000\u0001\u0000\ufffd\···\u0001D\u0000;&quot;}');

ERROR:  unsupported Unicode escape sequence
LINE 1: insert into logs values ('{&quot;resp&quot;:&quot;GIF89a\u0001\u0000\u0001\...
                                 ^
DETAIL:  \u0000 cannot be converted to text.
CONTEXT:  JSON data, line 1: {&quot;resp&quot;:...
</code></pre><p>Because in my code, there were only a couple of places where I didn&rsquo;t control the actual data, that went into a span, the way I’ve chosen to handle that was by introducing a wrapper type, that implements <code>json.Marshaller</code>. The wrapper checks the value is a valid UTF-8 sequence and doesn’t contain <code>NULL</code> character before it marshals the value into a JSON string. If the value is not a valid UTF-8, the marshaller sees it as a binary data and base64-encodes it.</p>
<pre tabindex=0><code>// RawText handles invalid UTF-8 and NULL-bytes, encoding them as base64-string.
// Because we have to make sure the resulting JSON will be compatible with Postgres's jsonb,
// we must use RawText when we don't control the data, e.g. when log the error from an external API.
// Refer to https://www.postgresql.org/docs/10/datatype-json.html
type RawText []byte

func (v RawText) MarshalEasyJSON(w *Writer) {
    if utf8.Valid(v) &amp;&amp; !bytes.ContainsRune(v, '\u0000') {
        // &quot;valid&quot; text is marshalled as string
        w.String(string(v))
    } else {
        // &quot;invalid&quot; text is marshalled as binary data
        w.Raw(json.Marshal([]byte(v)))
    }
}
</code></pre><p>Note, the code above is a marshaller for <a href=https://github.com/mailru/easyjson>github.com/mailru/easyjson</a>, which we use in the project.</p>
<p>Here is how it looks in practice:</p>
<pre tabindex=0><code>func main() {
    v, _ := json.Marshal(struct {
        Resp1 interface{} `json:&quot;resp1,omitempty&quot;`
        Resp2 interface{} `json:&quot;resp2,omitempty&quot;`
    }{
        Resp1: RawText(bin),             // wrap the bin data into RawText
        Rest2: RawText(&quot;normal string&quot;), // wrap (copy) a string into RawText
    })
    fmt.Printf(&quot;%s\n&quot;, v)
    // Output:
    // {
    // &quot;resp1&quot;:&quot;R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7&quot;,
    // &quot;resp2&quot;:&quot;normal string&quot;
    // }
}
</code></pre>
<footer class=meta>
<time datetime=2021-01-14T12:00:00Z>January 14, 2021</time><a class=tag href=/notes/postgresql/>PostgreSQL</a><a class=tag href=/notes/go/>Go</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=600 data-reading-time=3>
<header class=section-head>
<h2><a href=/notes/2020/12/2020-what-a-year/>2020, what a year</a></h2>
</header>
<p>2020 was (technically, still is) an extraordinary year. For many people, this was the worst, the weirdest, the most depressing year; the year we wish was a bad dream. With all the respect for the terrible things, lots of people went through, for me, it was a year to remember.</p>
<p>In 2020 I learned how to work from home. It’s still not super fun, but it’s possible. As I look at that now, there are obvious benefits: I can start and end my day much more freely; I currently don’t feel much pressure when willing to go for a walk or a run at 11:00, or when I finish my workday at 16:00 (I tend to start around 8:30 while at home). But that being said, I still don’t consider staying full-remote and I want to go back to the office, co-working, or a random coffee bar.</p>
<p>In 2020 I bought a bicycle. After three years in Berlin, I’m with “all of those people” now :) Can’t say I’m super excited about the purchase. Mostly because I live in the walking distance from the office. With the bicycle, my “commute” is only five or seven minutes shorter. I have to wait at the traffic lights a lot; plus I have to spend extra minutes to lock/unlock the bicycle.</p>
<p>In 2020 I walked, on average, one kilometre per day more, than in the previous year (5.5 km vs 4.2 km). A bit of surprise but, overall this year, I walked less than I did in 2019. I bet that’s because this year we, obviously, didn’t travel much, so we didn’t have a chance to spend weeks walking around a new city, neither in Spring, nor on Christmas and New Year.</p>
<p>In 2020 we adopted a cat. The CAT, who’s now called Gagarin! He came to us from the middle of “nowhere-Russia” as a frightened little kitten and spent the first month under our bed, avoiding walking under the light. And now, after four months, he comes to have a nap on our pillow (and on my head) at 7:00, as his way to signal that it’s time to feed him.</p>
<p>In 2020 I assembled my four nodes Raspberry Pi Kubernetes cluster, but I haven’t made much use of it yet. As promised to myself in December 2019, during the year I learned and touched some new Kubernetes details, and I have tons of things I still want to get my hands into to understand on a more in-depth level of details.</p>
<p>In 2020 I started to learn Rust. I tried learning it in 2018 and in 2019. But this year, I actually spent a month or two writing code, not only reading/watching/listening to/following the content. It feels refreshing. Programming for embedded devices and game-dev are the most attractive fields that I want to explore further with Rust.</p>
<p>In 2020 I had two job interviews. None of them were successful for me, although I tend to believe that I’ve passed both. I have plans to draft a note about the essential questions one must ask the HR during the first call, somewhere later.</p>
<p>In 2020 I did and learned, and read, and listened to, and watched, and thought about a bunch of things.</p>
<p>In 2021 I will do even better.</p>
<footer class=meta>
<time datetime=2020-12-29T12:00:00Z>December 29, 2020</time>
</footer>
</article>
<article class=article data-weight=0 data-word-count=200 data-reading-time=1>
<header class=section-head>
<h2><a href=/notes/2020/12/on-code-reviews/>On code-reviews</a></h2>
</header>
<p><em>This is another thought experiment, this time on the importance of my feedback in the code-reviews.</em></p>
<p>Providing you’re working on a project maintained by a set team of N people. What would happen with the codebase if, for six months, in code-reviews, you started to accept changes for which, you generally leave feedback?</p>
<p>&ldquo;Variables, struct or function names are named poorly&rdquo; — accept. &ldquo;We already have a package, that solves a similar problem&rdquo; — accept. &ldquo;A change brings inconsistency to the codebase&rdquo; — accept. &ldquo;The pattern doesn’t belong here&rdquo; — accept.</p>
<p>Go is a famously opinionated programming language. But does being opinionated help and scale outside of the language and its standard library?</p>
<p><em>Do you have the answer? Discuss this note on <a href=https://www.reddit.com/r/golang/comments/k7aqxw/a_thought_experiment_on_importance_of_your/>r/golang Reddit</a> or share your thoughts on <a href=https://twitter.com/tvii/status/1335303528064094210>Twitter</a>.</em></p>
<footer class=meta>
<time datetime=2020-12-05T12:00:00Z>December 5, 2020</time><a class=tag href=/notes/random/>random</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=200 data-reading-time=1>
<header class=section-head>
<h2><a href=/notes/2020/11/a-thought-experiment-on-apple-m1/>A thought experiment on Apple M1</a></h2>
</header>
<p>With Apple&rsquo;s new M1 Macs showing (<em>reportedly</em>) huge performance improvement, compared to &ldquo;old&rdquo;, Intel-based Macs, I wonder what would hold Qualcomm (Snapdragon CPU) and others from doing &ldquo;the same&rdquo; and moving into laptop/desktop territory?</p>
<p>Microsoft already has Surface Pro X — an ARM-based Windows computer. They also have a version of Win10 for ARM, that one can even run on Raspberry Pi (still beta quality, I believe). Could 2021 become the year of ARM on desktop?</p>
<p>Even more interesting, Amazon&rsquo;s Graviton is showing (<em>again, reportedly</em>) an excellent performance, while staying at reasonably low cost. What would stop Google/Microsoft from moving into ARM-based CPU territory for their clouds?</p>
<footer class=meta>
<time datetime=2020-11-24T12:00:00Z>November 24, 2020</time><a class=tag href=/notes/random/>random</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=200 data-reading-time=1>
<header class=section-head>
<h2><a href=/notes/2020/09/vscode-font-variant/>Alternative font-variant in VS Code</a></h2>
</header>
<p>I&rsquo;m not a big fun of the aesthetics of <a href=https://code.visualstudio.com/>VS Code</a>, at least not on macOS. In particular, I don&rsquo;t like the way how its code editor renders fonts. But today I learned!</p>
<p>To make the fonts look better — I&rsquo;m on macOS — set an alternative font-variant, e.g. &ldquo;Comic Code Ligatures Medium&rdquo; instead of &ldquo;Regular&rdquo;, in the editor&rsquo;s settings. To specify font-variant, remove spaces in between the name of the font and pass the variant after the hyphen. That is, instead of <code>'Comic Code Ligatures'</code>, set the following:</p>
<pre tabindex=0><code>&quot;editor.fontFamily&quot;: &quot;'ComicCodeLigatures-Medium', monospace&quot;
</code></pre><hr>
<p><em>Full disclosure, my IDE of choice is <a href=https://www.jetbrains.com/go/>GoLand</a>, and it has been so since it was only a plugin for IntelliJ IDEA. Even though I can comfortably use VS Code or Vim when I need to make a small change or look something up in the code, I need the IDE to effectively work on a large codebase.</em></p>
<p>The backstory for this note is that I&rsquo;m working on a small TypeScript/React application in my spare time this month. For something that small, VS Code works <del>great</del> fine. In fact, I&rsquo;m writing this particular note in VS Code too ;)</p>
<footer class=meta>
<time datetime=2020-09-09T12:00:00Z>September 9, 2020</time><a class=tag href=/notes/vs-code/>vs code</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=100 data-reading-time=1>
<header class=section-head>
<h2><a href=/notes/2020/09/one-year-in-production/>One year in production</a></h2>
</header>
<p>It&rsquo;s one year since I posted the very first note here. Unbelivable! Despite my own concerns I did published random posts over the course of the previous twelve months.</p>
<p>For the next round I came up with some personal goals:</p>
<ol>
<li>Keep posting.</li>
<li>Work on your grammar.</li>
<li>Add &ldquo;Archive&rdquo; and &ldquo;Tags&rdquo; sections.</li>
<li>Find a better approach for managing drafts.</li>
<li>Bring back the dark theme but figure out what to do with the illustrations.</li>
<li>Keep posting ;)</li>
</ol>
<footer class=meta>
<time datetime=2020-09-01T12:00:00Z>September 1, 2020</time>
</footer>
</article>
<article class=article data-weight=0 data-word-count=600 data-reading-time=3>
<header class=section-head>
<h2><a href=/notes/2020/08/does-profefe-prefers-push-over-pull/>Does profefe prefers "push" over "pull"?</a></h2>
</header>
<p>The main component of <a href=https://github.com/profefe/profefe>profefe, a system for continuous profiling</a>, is <em>profefe-collector</em> — a service that receives profiling data, taken from an application and persists the data in collector’s storage (<a href=https://github.com/profefe/profefe/blob/master/DESIGN.md>design document</a> describes it in more details). Receiving data from an external source (for example, <em>profefe-agent</em>), indicates that profefe, as an observability system, prefers &ldquo;push&rdquo; model. Why not &ldquo;pulling&rdquo; the profiles directly from the application?</p>
<p>Both push and pull models have their benefits and drawbacks.</p>
<p>A collector that pulls profiling data from running applications could simplify integration into existing infrastructure because there would be no need in making changes in the applications that already exposed pprof HTTP endpoint. Making sure that every application integrated and configured profefe-agent would be a challenging job in a large organisation.</p>
<p>On the other hand, pull model requires pprof servers to be exposed and available for the collector, so it could fetch (<em>pull</em>) profiling data. That can also be challenging in the deployments, where applications are collocated on the bare-metal machines. Every application (application&rsquo;s instance) would have to communicate a unique TCP port for its pprof server.</p>
<p>To work as a pull-system, the collector must be able to discover the pprof servers, thus it requires a mechanism for service discovery (SD), to be usable at scale. Unfortunately, there isn&rsquo;t a universal SD protocol or a provider, an observability system could be built upon.</p>
<p>Prometheus, the best example of an open-source system, which uses pull model for data collection, have to support several different SD systems in their code base. At some point they <a href=https://prometheus.io/blog/2018/07/05/implementing-custom-sd/>ended up introducing their own general protocol</a>, that expects a &ldquo;middle-man-service&rdquo;, which translates the data from a SD system into a list of Prometheus targets (<em>Update, <a href="https://www.reddit.com/r/golang/comments/idwi4d/does_profefe_a_system_for_continuous_profiling/g2by237/?utm_source=reddit&utm_medium=web2x&context=3">this comment from u/bbrazil</a> does a better job explaining the state of SD in Prometheus</em>). There is no clear way for an open-source system to be both flexible and don&rsquo;t end up being a pile of &ldquo;plugins&rdquo;, that no one is willing to maintain or break.</p>
<p>From the start of profefe project, several years ago, I had the idea that translating push into pull would be easier for an end-user. That is if a small deployment already exposes a pprof server, writing an external job that pulls the profiles from the applications, annotates them with meta-data, and pushes the data into the collector, can be as easy as spawning a cronjob in a sidecar. <a href=https://github.com/profefe/kube-profefe>kube-profefe</a> solves that nicely for deployments running in Kubernetes. At some point, I hoped to come up with something similar for Nomad+Consul if the experiments ended successfully.</p>
<p>Translating pull into push is a similarly possible but because profefe didn’t have to support any SD mechanisms from the start, that simplified the overall code base and allowed us to focus on the collector and the <a href=https://github.com/profefe/profefe#http-api>API for profiles quering</a>.</p>
<p>profefe-collector does uses push model. But one can deploy profefe so it reflected the use cases your organisation has.</p>
<p>Do you use continuous profiling? Let me know about your experience. Share your thoughts <a href=https://twitter.com/tvii/status/1296796503781122049>on Twitter</a> or <a href=https://www.reddit.com/r/golang/comments/idwi4d/does_profefe_a_system_for_continuous_profiling/>discuss on r/golang</a>.</p>
<footer class=meta>
<time datetime=2020-08-21T12:00:00Z>August 21, 2020</time><a class=tag href=/notes/profefe/>profefe</a><a class=tag href=/notes/continuous-profiling/>continuous profiling</a><a class=tag href=/notes/go/>Go</a><a class=tag href=/notes/pprof/>pprof</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=100 data-reading-time=1>
<header class=section-head>
<h2><a href=/notes/2020/08/how-to-design-a-good-api/>How to design a good API</a></h2>
</header>
<p>A good API is designed around the use-case. A poorly designed, around the API&rsquo;s implementation details.</p>
<footer class=meta>
<time datetime=2020-08-14T12:00:00Z>August 14, 2020</time><a class=tag href=/notes/random/>random</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=300 data-reading-time=2>
<header class=section-head>
<h2><a href=/notes/2020/07/whats-in-your-main-dot-go/>What's in your main-dot-go? (aka New Go Project Boilerplate)</a></h2>
</header>
<p>Sometimes I write small services in Go from scratch. And every time <code>main.go</code> ends up looking almost the same:</p>
<pre tabindex=0><code>func main() {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	sigs := make(chan os.Signal, 2)
	signal.Notify(sigs, os.Interrupt, syscall.SIGTERM)

	go func() {
		&lt;-sigs
		signal.Stop(sigs)
		cancel()
	}()

	if err := run(ctx, os.Args[1:]); err != nil {
		log.Fatalln(err)
	}
}

type Config struct {
	HTTPAddr            string
	HTTPShutdownTimeout time.Duration
}

func run(ctx context.Context, args []string) error {
	flags := flag.NewFlagSet(&quot;&quot;, flag.ExitOnError)

	var conf Config
	flags.StringVar(&amp;conf.HTTPAddr, &quot;http-addr&quot;, &quot;127.0.0.1:10080&quot;, &quot;address to listen on&quot;)
	flags.DurationVar(&amp;conf.HTTPShutdownTimeout, &quot;http-shutdown-timeout&quot;, 5*time.Second, &quot;server shutdown timeout&quot;)

	if err := flags.Parse(args); err != nil {
		return err
	}

	// TODO: define the handler, the routing, and wire the dependencies with the main context
	mux := http.NewServeMux()
	server := &amp;http.Server{
		Addr:    conf.HTTPAddr,
		Handler: mux,
	}

	errs := make(chan error, 1)
	go func() {
		log.Printf(&quot;starting: addr %s&quot;, server.Addr)
		errs &lt;- server.ListenAndServe()
	}()

	select {
	case &lt;-ctx.Done():
		log.Println(&quot;exiting...&quot;)
	case err := &lt;-errs:
		return err
	}

	// create new context because top-most one is already canceled
	ctx, cancel := context.WithTimeout(context.Background(), conf.HTTPShutdownTimeout)
	defer cancel()

	return server.Shutdown(ctx)
}
</code></pre><p>Of course, not every service requires an HTTP server, but the general idea stands.</p>
<hr>
<p>When Go will introduce <a href=https://github.com/golang/go/issues/37255><code>signal.NotifyContext()</code></a>, the signals handling in <code>main()</code> function will be much smaller (we&rsquo;re at go1.15rc1 as I&rsquo;m writing that and the change hasn&rsquo;t landed into the release yet).</p>
<hr>
<p>I love how transparent is the flow here and how everything is scoped inside <code>run()</code> function. This structure forces you to eliminate global state, making unit or integration testing <em>almost trivial</em> — at least, in theory ;)</p>
<p>It might feel like too much of boilerplate code for a &ldquo;small&rdquo; service. In practice, though, I don&rsquo;t recall any time this caused any real troubles to me. The beauty of Go is in explicitness.</p>
<footer class=meta>
<time datetime=2020-07-31T12:00:00Z>July 31, 2020</time><a class=tag href=/notes/go/>Go</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=200 data-reading-time=1>
<header class=section-head>
<h2><a href=/notes/2020/07/waveshare-esp8266-driver-board-pins-mapping/>Waveshare ESP8266 Driver Board Pins Mapping</a></h2>
</header>
<p>I&rsquo;ve been playing with an <a href=https://www.waveshare.com/product/iot-communication/short-range-wireless/wifi/e-paper-esp8266-driver-board.htm>e-paper ESP866 Driver Board</a>, and a <a href=https://www.waveshare.com/2.7inch-e-Paper.htm>2.7" E-Ink display</a> from Waveshare. Arduino C++ looks manageable. One strange thing, though. In both board’s documentation and <a href=https://github.com/ZinggJM/GxEPD2>GxEPD2</a> library’s examples, they say the display is connected to pins as BUSY → GPIO16, RST → GPIO5, DC → GPIO4, CS → GPIO15. This mapping seems wrong.</p>
<p>After digging through the code examples from <a href=https://github.com/ZinggJM/GxEPD2>Waveshare’s Wiki</a>, the correct mapping is the following:</p>
<p>BUSY → GPIO5,
RST → GPIO2,
DC → GPIO4,
CS → GPIO15</p>
<p>That&rsquo;s how the initialisation of the main <code>GxEPD2</code> class for my 2.7" display looks like now:</p>
<pre tabindex=0><code>#define ENABLE_GxEPD2_GFX 0
#include &lt;GxEPD2_BW.h&gt;

// mapping of Waveshare e-Paper ESP8266 Driver Board
GxEPD2_BW&lt;GxEPD2_270, GxEPD2_270::HEIGHT&gt; display(GxEPD2_270(/*CS=15*/ SS, /*DC=4*/ 4, /*RST=2*/ 2, /*BUSY=5*/ 5));
</code></pre>
<footer class=meta>
<time datetime=2020-07-14T12:00:00Z>July 14, 2020</time><a class=tag href=/notes/homelab/>homelab</a><a class=tag href=/notes/waveshare/>waveshare</a><a class=tag href=/notes/esp8266/>esp8266</a><a class=tag href=/notes/e-paper/>e-paper</a><a class=tag href=/notes/arduino/>arduino</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=300 data-reading-time=2>
<header class=section-head>
<h2><a href=/notes/2020/06/sticky-headers/>Sticky headers? Please don't</a></h2>
</header>
<p>Sticky (or &ldquo;<em>fixed</em>") headers are everywhere. It feels that, nowadays, every web designer’s first attempt to site&rsquo;s navigation starts with a sticky header. I hate this.</p>
<p><a href=/notes/2020/06/sticky-headers/>Keep reading, the note has images…</a></p>
<footer class=meta>
<time datetime=2020-06-17T12:00:00Z>June 17, 2020</time><a class=tag href=/notes/design/>design</a><a class=tag href=/notes/random/>random</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=300 data-reading-time=2>
<header class=section-head>
<h2><a href=/notes/2020/04/owner-of-logging-context/>Owner of Logging Context</a></h2>
</header>
<p>There’s the late-night dilemma&mldr;</p>
<p><strong>Who should be in charge of logging context: a component&rsquo;s owner or the component itself?</strong></p>
<pre tabindex=0><code>type Logger interface {
	With(...kvpairs) Logger
}

type Storage struct {
	logger Logger
}

// OPTION 1: component's owner defines the context of component's logger
func main() {
	_ = NewStorage(logger.With(&quot;component&quot;, &quot;storage&quot;))
}

// OPTION 2: component itself is in charge of its logging context
func NewStorage(logger Logger) (st *Storage) {
	return &amp;Storage{
		logger: logger.With(&quot;component&quot;, &quot;storage&quot;),
	}
}
</code></pre><p>Fun fact: a couple months back, we ruined the team&rsquo;s Friday, by debating about a similar topic in the context of (Graphite) metrics namespaces. It has become even more intricate since then :/</p>
<p><strong>Update (2020-04-15)</strong></p>
<p>Many people <a href=https://twitter.com/tvii/status/1250166077235048449>on Twitter</a> suggest that <em>Option 1</em> is an obvious choice because only application knows how to name the components. I totally agree with that.</p>
<p>As I <a href=https://twitter.com/tvii/status/1250298527814529026>wrote later</a>, the <em>real</em> dilemma is not about &ldquo;application and component&rdquo; but about &ldquo;owner of the component&rdquo;. Function <code>main</code>, in the example above, was a silly example, that tried (and failed) to illustrate the question in a code.</p>
<p>Let&rsquo;s try another (<em>silly</em>) example:</p>
<pre tabindex=0><code>// there are buch of different handlers (maybe ten) in this application
type Handler1 struct { logger Logger }

func (h *Handler1) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	// OPTION 1
	req := NewRequst(h.logger.With(&quot;component&quot;, &quot;request&quot;), r)
}

type Handler2 struct { logger Logger }

func (h *Handler2) ServeHTTP(w http.ResponseWriter, r *http.Request) {
	// OPTION 1, still
	req := NewRequst(h.logger.With(&quot;component&quot;, &quot;request&quot;), r)
}

type Request struct {
	logger Logger
}

// OPTION 2
func NewRequst(logger Logger, *r http.Request) *Request {
	return &amp;Request{
		logger: logger.With(&quot;component&quot;, &quot;request&quot;),
	}
}
</code></pre><p>We want to have a consistent nomenclature across the application&rsquo;s logs.</p>
<p>Is the choice still obvious? ;)</p>
<p><em>Do you have an opinion? Share it with me <a href=https://twitter.com/tvii/status/1250166077235048449>on Twitter</a></em>.</p>
<footer class=meta>
<time datetime=2020-04-14T12:00:00Z>April 14, 2020</time>
</footer>
</article>
<article class=article data-weight=0 data-word-count=1100 data-reading-time=5>
<header class=section-head>
<h2><a href=/notes/2020/03/go-osx-core-location/>Retrieve Location of macOS Device from Go</a></h2>
</header>
<p>Participating in self-isolation is more fun when you have toys to play. As a fun weekend project, I wanted to look at how one accesses macOS Location Services and get the geographic location of the device from Go.</p>
<p>To obtain the geographic location of a device on macOS, we use <a href="https://developer.apple.com/documentation/corelocation?language=objc">Apple’s Core Location</a> framework. The framework is part of the OS, but it requires writting Objective-C (<em>or Swift</em>). Thanks to Go&rsquo;s cgo and because Objective-C is from the family of C languages, we can write a bridge between Objective-C and Go.</p>
<p><a href=/notes/2020/03/go-osx-core-location/>Keep reading… </a></p>
<footer class=meta>
<time datetime=2020-03-30T12:00:00Z>March 30, 2020</time><a class=tag href=/notes/go/>Go</a><a class=tag href=/notes/cgo/>cgo</a><a class=tag href=/notes/objective-c/>objective-c</a><a class=tag href=/notes/macos/>macos</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=1200 data-reading-time=6>
<header class=section-head>
<h2><a href=/notes/2020/01/buildkit-multi-platform-travis-ci/>Building Multi-Platform Docker Images with Travis CI and BuildKit</a></h2>
</header>
<p><em>This is a lengthy note. If you don&rsquo;t quite feel reading and only need the working example, go directly to <a href=https://github.com/profefe/profefe/blob/09ff03be6561a7ef88fba7b96d923abd6a413931/.travis.yml>the Travis CI build file</a>.</em></p>
<p>The more I delve into the world of Raspberry Pi, the more I notice that &ldquo;regular and boring&rdquo; things on ARM are harder than I expected.</p>
<p>People build and distribute software exclusively for amd64. You read another &ldquo;<em>Kubernetes something</em>&rdquo; tutorial, that went viral on Twitter, and is fancy to try it out. Still, all helm charts, or whatever the author prefered, use Docker images built exclusively for amd64.</p>
<p>Docker toolchain has added the support for building multi-platform images in 19.x. However, it&rsquo;s available only under the &ldquo;experimental&rdquo; mode. The topic of building multi-platform Docker images yet feels underrepresented.</p>
<h2 id=but-first-what-are-multi-platform-docker-images>But first, what are multi-platform Docker images?</h2>
<p>When a client, e.g. Docker client, tries to pull an image, it must negotiate the details about what exactly to pull with the registry. The registry provides a manifest that describes the digest of the requested image, the volumes the image consists of, the platform this image can run on, etc. Optionally, the registry can provide a manifests list, which, as the name suggests, is a list of several manifests bundled into one. With the manifests list in hands, the client can figure out the particular digest of the image it needs to pull.</p>
<p>So multi-platform Docker images are just several images, whose manifests are bundled into the manifests list.</p>
<p>Imagine we want to pull the image <a href=https://hub.docker.com/_/golang><code>golang:1.13.6-alpine3.10</code></a>. Docker client will get the manifests list from Dockerhub. This list includes digests of several images, each built for the particular platform. If we&rsquo;re on Raspberry Pi, running the current Raspbian Linux, which is <code>arm/v7</code>, the client will pick the corresponding image&rsquo;s digest. Alternatively, we could choose to pull the image <a href=https://hub.docker.com/r/arm32v7/golang/><code>arm32v7/golang:1.13.6-alpine3.10</code></a> instead, and we ended up with the same image with the <a href=https://hub.docker.com/layers/arm32v7/golang/alpine3.10/images/sha256-d72fa60fb5b9ffc12db9c87340bc9d9f55852570f1efc7d7656f081749a7f0aa>digest <code>d72fa60fb5b9</code></a>. Of course, to use a single universal image name, i.e. <code>golang</code>, on every platform is way more convenient.</p>
<p>You can read more about manifests <a href=https://docs.docker.com/registry/spec/manifest-v2-2/>in Docker registry documentation</a>.</p>
<h2 id=does-it-mean-i-need-to-build-different-docker-images-for-each-platform-i-want-to-support>Does it mean I need to build different Docker images, for each platform I want to support?</h2>
<p>Well, yes. This is how, official images are built.</p>
<p>For every platform, the image is built and pushed to the registry under the name <code>&lt;platform>/&lt;image>:&lt;tag></code>, e.g. <a href=https://hub.docker.com/r/amd64/golang/><code>amd64/golang:1-alpine</code></a>. And next, a manifests list, that combines all those platform-specific images, is built and pushed with the simple name <code>&lt;image>:&lt;tag></code>.</p>
<p><a href=https://github.com/moby/buildkit>Docker&rsquo;s BuildKit</a> provides a toolkit that, among other nice things, allows building multi-platform images on a single host. BuildKit is used inside <a href=https://github.com/docker/buildx>Docker' buildx project</a>, that is part of the recent Docker version.</p>
<p>One can use buildx, but, for this post, I wanted to try out, what would it look like to use BuildKit directly. For <a href=https://github.com/profefe/profefe>profefe</a>, the system for continuous profiling of Go services, I set up <a href=https://travis-ci.com/>Travis CI</a>, that builds a multi-platform Docker image and pushes them to Dockerhub.</p>
<p>profefe is written in Go. That simplifies things, because, thanks to Go compiler, I don&rsquo;t have to think about how to compile code for different platforms. <a href=https://github.com/profefe/profefe/blob/09ff03be6561a7ef88fba7b96d923abd6a413931/contrib/docker/Dockerfile>The same Dockerfile</a> will work fine on every platform.</p>
<p>Here&rsquo;s how &ldquo;deploy&rdquo; stage of the build job looks like (<a href=https://github.com/profefe/profefe/blob/09ff03be6561a7ef88fba7b96d923abd6a413931/.travis.yml>see <code>travis.yml</code> on profefe&rsquo;s GitHub</a>).</p>
<pre tabindex=0><code>dist: bionic

language: go
go:
  - 1.x

jobs:
  include:
    - stage: deploy docker
      services: docker
      env:
        - PLATFORMS=&quot;linux/amd64,linux/arm64,linux/arm/v7&quot;
      install:
        - docker container run --rm --privileged multiarch/qemu-user-static --reset -p yes
        - docker container run -d --rm --name buildkitd --privileged moby/buildkit:latest
        - sudo docker container cp buildkitd:/usr/bin/buildctl /usr/local/bin/
        - export BUILDKIT_HOST=&quot;docker-container://buildkitd&quot;
      script: skip
      deploy:
        - provider: script
          script: |
            buildctl build \
              --progress=plain \
              --frontend=dockerfile.v0 \
              --local context=. --local dockerfile=. \
              --opt filename=contrib/docker/Dockerfile \
              --opt platform=$PLATFORMS \
              --opt build-arg:VERSION=\&quot;master\&quot; \
              --opt build-arg:GITSHA=\&quot;$TRAVIS_COMMIT\&quot; \
              --output type=image,\&quot;name=profefe/profefe:git-master\&quot;,push=true
          on:
            repo: profefe/profefe
            branch: master
      before_deploy:
        - echo &quot;$DOCKER_PASSWORD&quot; | docker login --username &quot;$DOCKER_USERNAME&quot; --password-stdin
      after_failure:
        - buildctl debug workers ls
        - docker container logs buildkitd
</code></pre><p>It&rsquo;s a lot happening here, but I&rsquo;ll describe the most critical parts.</p>
<p>Let&rsquo;s start with <code>dist: bionic</code>.</p>
<p>We run the builds under Ubuntu 18.04 (Bionic Beaver). To be able to build multi-platform images on a single amd64 host, BuildKit uses QEMU to emulate other platforms. That requires Linux kernel 4.8, so even Ubuntu 16.04 (Xenial Xerus) should work.</p>
<p>The top-level details on how the emulation works are very well described in <a href=https://www.kernel.org/doc/html/latest/admin-guide/binfmt-misc.html>https://www.kernel.org/doc/html/latest/admin-guide/binfmt-misc.html</a></p>
<p>In short, we tell the component of the kernel (<code>binfmt_misc</code>) to use QEMU when the system executes a binaries built for a different platform. The following call in the &ldquo;install&rdquo; step is what&rsquo;s doing that:</p>
<pre tabindex=0><code>- docker container run --rm --privileged multiarch/qemu-user-static --reset -p yes
</code></pre><p>Under the hood, the container runs a <a href=https://raw.githubusercontent.com/qemu/qemu/master/scripts/qemu-binfmt-conf.sh>shell script from QEMU project</a>, that registers the emulator as an executor of binaries from the external platforms.</p>
<blockquote>
<p>If you think, that running a docker container to do the manipulations with the host&rsquo;s OS looks weird, well&mldr; I can&rsquo;t agree more. Probably, a better approach would be to install <a href=https://packages.ubuntu.com/bionic/qemu-user-static>qemu-user-static</a>, which would do the proper setup. Unfortunately, the current package&rsquo;s version for Ubuntu Bionic doesn&rsquo;t do the registration as we need it. I.e. its post-install doesn&rsquo;t add the <code>"F"</code> flag (&ldquo;fix binaries&rdquo;), which is crucial for our goal. Let&rsquo;s just agree,that docker-run will do ok for the demonstrational purpose.</p>
</blockquote>
<pre tabindex=0><code>- docker container run -d --rm --name buildkitd --privileged moby/buildkit:latest
- sudo docker container cp buildkitd:/usr/bin/buildctl /usr/local/bin/
- export BUILDKIT_HOST=&quot;docker-container://buildkitd&quot;
</code></pre><p>This is another &ldquo;docker-run&rsquo;ism&rdquo;. We start BuildKit&rsquo;s <code>buildkitd</code> daemon inside the container, attaching it to the Docker daemon that runs on the host (&ldquo;privileged&rdquo; mode). Next, we copy <code>buildctl</code> binary from the container to the host system and set <code>BUILDKIT_HOST</code> environment variable, so <code>buildctl</code> knew where its daemon runs.</p>
<blockquote>
<p>Alternatively, we could install BuildKit from GitHub and run the daemon directly on the build host. YOLO.</p>
</blockquote>
<pre tabindex=0><code>before_deploy:
  - echo &quot;$DOCKER_PASSWORD&quot; | docker login --username &quot;$DOCKER_USERNAME&quot; --password-stdin
</code></pre><p>To be able to push the images to the registry, we need to log in providing Docker credentials to host&rsquo;s Docker daemon. The credentials are set as Travis CI&rsquo;s encrypted environment variables ([refer to Travis CI docs])](<a href=https://docs.travis-ci.com/user/environment-variables/))>https://docs.travis-ci.com/user/environment-variables/))</a>.</p>
<pre tabindex=0><code>buildctl build \
  --progress=plain \
  --frontend=dockerfile.v0 \
  --local context=. --local dockerfile=. \
  --opt filename=contrib/docker/Dockerfile \
  --opt platform=$PLATFORMS \
  --opt build-arg:VERSION=\&quot;master\&quot; \
  --opt build-arg:GITSHA=\&quot;$TRAVIS_COMMIT\&quot; \
  --output type=image,\&quot;name=profefe/profefe:git-master\&quot;,push=true
</code></pre><p>This is the <em>black box</em> where everything happens. Magically!</p>
<p>We run <code>buildctl</code> stating that it must use the specified Dockerfile; it must build the images for defined platforms (I specified <code>linux/amd64,linux/arm64,linux/arm/v7</code>), create a manifests list tagged as the desired image (<code>profefe/profefe:&lt;version></code>), and push all the images to the registry.</p>
<p><code>buildctl debug workers ls</code> shows what platforms does BuildKit on this host support. I listed only those I&rsquo;m currently intrested with.</p>
<p>And that&rsquo;s all. This setup automatically builds and pushes multi-platform Docker images for profefe (<a href=https://hub.docker.com/p/profefe/profefe>https://hub.docker.com/p/profefe/profefe</a>) on a commit to project&rsquo;s &ldquo;master&rdquo; branch on GitHub.</p>
<hr>
<p>As I hope you&rsquo;ve seen, support for multi-platform is getting easier and things that were hard a year ago are only mildly annoying now :)</p>
<p>If you have any comments or suggestions, reach out to me on <a href=https://twitter.com/tvii/status/1221858810006065154>Twitter</a> or discuss this note on <a href=https://www.reddit.com/r/docker/comments/eutrkg/building_multiplatform_docker_images_with_travis/>r/docker Reddit</a>.</p>
<p>Some more reading on the topic:</p>
<ul>
<li><a href=https://github.com/moby/buildkit>Documentation for BuildKit project</a></li>
<li><a href=https://github.com/docker/buildx#building-multi-platform-images>Building multi-platform images with docker and buildx</a></li>
<li><a href=https://www.docker.com/blog/docker-official-images-now-multi-platform/>Docker Official Images are now Multi-platform, Docker official announcement</a></li>
</ul>
<footer class=meta>
<time datetime=2020-01-27T12:00:00Z>January 27, 2020</time><a class=tag href=/notes/docker/>docker</a><a class=tag href=/notes/buildkit/>buildkit</a><a class=tag href=/notes/travis-ci/>travis ci</a><a class=tag href=/notes/raspberry-pi/>raspberry pi</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=700 data-reading-time=4>
<header class=section-head>
<h2><a href=/notes/2020/01/raspi-ubuntu-arm64-k3s/>k3s with Ubuntu Server (arm64) on Raspberry Pi 4</a></h2>
</header>
<p>As I&rsquo;ve <a href="https://twitter.com/tvii/status/1215927299557797893?s=20">twitted</a> recently, I&rsquo;m updating one of my Raspberry Pis to <a href=https://ubuntu.com/download/raspberry-pi>Ubuntu Server 19.10 (arm64)</a>.</p>
<h2 id=one-of-raspberry-pis>&ldquo;One of Raspberry Pis&rdquo;?</h2>
<p>My home cluster is four <a href="https://www.raspberrypi.org/products/raspberry-pi-4-model-b/?variant=raspberry-pi-4-model-b-2gb">Raspberry Pis 4 (2GB)</a>; all connected to my internet router through ethernet and
powered with <a href="https://www.amazon.de/gp/product/B00PTLSH9G/ref=ppx_yo_dt_b_asin_title_o03_s00?ie=UTF8&psc=1">60W 6 USB-ports charger</a>. All Pis build a small kubernetes cluster that runs with <a href=https://k3s.io/>k3s</a>.</p>
<p>All by one Pis run on <a href=https://www.raspberrypi.org/downloads/raspbian/>Raspbian Buster Lite</a> and this setup&rsquo;s been working pretty well until I&rsquo;ve found out,
<a href=https://www.aerospike.com/>Aerospike</a>, a database I required to run for a testing lab, only works on a 64-bit OS.</p>
<p>Luckily, <a href=https://ubuntu.com/download/raspberry-pi>Ubuntu Server has an arm64 version</a> built for Raspberry Pi. Thus, my working plan is to switch one Pi
to Ubuntu, compile and run a single-instance Aerospike server (<em>and any other components, that require a 64-bit OS</em>) on this Pi, and provide a kubernetes service in front of the DB, so other components in the cluster could access it as if it was
fully managed by kubernetes.</p>
<h2 id=the-setup>The Setup</h2>
<p>Setting up Ubuntu Server on a Pi was smooth. All I did was flushing the image with 19.10 OS to an SD card, as described in <a href=https://ubuntu.com/download/iot/installation-media>Ubuntu wiki</a>. That is, the headless setup worked out of the box, and after I inserted the card into the PI and connected it to the router, I managed to SSH into the system:</p>
<pre tabindex=0><code>$ ssh ubuntu@192.168.10.18
</code></pre><p>The default password for <code>ubuntu</code> user is <code>ubuntu</code>. The system asks to change the password on the first login.</p>
<p>The first thing to do after installing the system:</p>
<pre tabindex=0><code>$ sudo apt-get update
$ sudo apt-get upgrade -y
</code></pre><p>Disable &ldquo;message of the day&rdquo; (<code>motd</code>) to speed SSH login.
For that I commented out the following lines in <code>/etc/pam.d/login</code> and <code>/etc/pam.d/sshd</code>:</p>
<pre tabindex=0><code>#session    optional     pam_motd.so  motd=/run/motd.dynamic
#session    optional     pam_motd.so noupdate
</code></pre><p>Reduce GPU memory split. <em>I truly don&rsquo;t know if that even makes sense, tbh; read about memory split on <a href=https://www.raspberrypi.org/documentation/configuration/config-txt/memory.md>Raspberry PI config-txt wiki</a>.</em> I added the following to <code>/boot/firmware/usercfg.txt</code>:</p>
<pre tabindex=0><code>gpu_mem=16
</code></pre><p>To run Kubernetes or Docker, the kernel needs some cgroup options. On Ubuntu Server, the configuration is in <code>/boot/firmware/nobtcmd.txt</code> (<em>refer to <code>cmdline=nobtcmd.txt</code> in <code>/boot/firmware/nobtcfg.txt</code></em>). Add the following to the end of the file:</p>
<pre tabindex=0><code>cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1
</code></pre><p>Reboot the Pi, re-login, and all is ready to <a href=https://rancher.com/docs/k3s/latest/en/installation/>install k3s-agent</a>:</p>
<pre tabindex=0><code>$ curl -sfL https://get.k3s.io  | K3S_URL=&quot;https://&lt;k3s-master-pi&gt;:6443&quot; K3S_TOKEN=&quot;&lt;k3s-token&gt;&quot; sh -
</code></pre><p>After the agent installed and running, check the Pi was added to kubernetes cluster:</p>
<pre tabindex=0><code>pi@pi-1:~ $ sudo kubectl get node -o wide
NAME   ROLES    AGE   VERSION         INTERNAL-IP     OS-IMAGE                         KERNEL-VERSION      CONTAINER-RUNTIME
pi-1   master   30d   v1.17.0+k3s.1   192.168.10.16   Raspbian GNU/Linux 10 (buster)   4.19.75-v7l+        containerd://1.3.0-k3s.5
pi-2   &lt;none&gt;   47h   v1.17.0+k3s.1   192.168.10.14   Raspbian GNU/Linux 10 (buster)   4.19.75-v7l+        containerd://1.3.0-k3s.5
pi-3   &lt;none&gt;   47h   v1.17.0+k3s.1   192.168.10.15   Raspbian GNU/Linux 10 (buster)   4.19.75-v7l+        containerd://1.3.0-k3s.5
pi-4   &lt;none&gt;   10h   v1.17.0+k3s.1   192.168.10.18   Ubuntu 19.10                     5.3.0-1014-raspi2   containerd://1.3.0-k3s.5
</code></pre><p>That is for today. The next steps are to figure out how to <a href=https://github.com/aerospike/aerospike-server/pull/6>build Aerospike on arm64</a>, but this is a story for another day.</p>
<h3 id=update-2020-01-15>Update (2020-01-15)</h3>
<p>I&rsquo;ve managed to build and run Aerospike server for arm64! See <a href=https://github.com/narqo/aerospike-server/tree/make-arm64v8>make-arm64v8 branch</a>
in my fork of aerospike-server and <a href=https://gist.github.com/narqo/98e95760852cee9a64c3557026b3fe4a>the gist</a> with my systemd services and configs.</p>
<h3 id=update-2020-02-13>Update (2020-02-13)</h3>
<p>A week ago I tried to install Ubuntu Server 18.04.3 on Pi 4 and didn&rsquo;t even get to the login shell in the headless mode.
Now <a href=https://ubuntu.com/download/raspberry-pi>Ubuntu Server 18.04.4 LTS</a> is out and it works exactly as described in this note:</p>
<pre tabindex=0><code>$ kubectl get node -o wide
NAME   ROLES    AGE     VERSION        INTERNAL-IP     OS-IMAGE                         KERNEL-VERSION      CONTAINER-RUNTIME
pi-1   master   63d     v1.17.2+k3s1   192.168.10.16   Raspbian GNU/Linux 10 (buster)   4.19.93-v7l+        containerd://1.3.3-k3s1
pi-2   &lt;none&gt;   35d     v1.17.2+k3s1   192.168.10.14   Raspbian GNU/Linux 10 (buster)   4.19.93-v7l+        containerd://1.3.3-k3s1
pi-3   &lt;none&gt;   14m     v1.17.2+k3s1   192.168.10.20   Ubuntu 18.04.4 LTS               5.3.0-1017-raspi2   containerd://1.3.3-k3s1
pi-4   &lt;none&gt;   4d23h   v1.17.2+k3s1   192.168.10.19   Ubuntu 19.10                     5.3.0-1017-raspi2   containerd://1.3.3-k3s1
</code></pre><h3 id=update-2020-06-01>Update (2020-06-01)</h3>
<p>Ubuntu Server 20.04 is the recommended version of Ubuntu for Ri 4. It works perfectly fine:</p>
<pre tabindex=0><code>$ kubectl get node -o wide
NAME   ROLES    AGE     VERSION        INTERNAL-IP     OS-IMAGE                         KERNEL-VERSION     CONTAINER-RUNTIME
pi-1   master   172d    v1.18.2+k3s1   192.168.10.41   Raspbian GNU/Linux 10 (buster)   4.19.118-v7l+      containerd://1.3.3-k3s2
pi-2   &lt;none&gt;   57d     v1.17.4+k3s1   192.168.10.42   Raspbian GNU/Linux 10 (buster)   4.19.118-v7l+      containerd://1.3.3-k3s2
pi-3   &lt;none&gt;   38d     v1.17.4+k3s1   192.168.10.43   Ubuntu 20.04 LTS                 5.4.0-1011-raspi   containerd://1.3.3-k3s2
pi-4   &lt;none&gt;   3d19h   v1.18.2+k3s1   192.168.10.44   Ubuntu 20.04 LTS                 5.4.0-1011-raspi   containerd://1.3.3-k3s2
</code></pre><p>Also Raspberry Pi OS (<em>ex-Raspbian</em>) for arm64 is currently in beta.</p>
<footer class=meta>
<time datetime=2020-01-11T12:00:00Z>January 11, 2020</time><a class=tag href=/notes/homelab/>homelab</a><a class=tag href=/notes/raspberry-pi/>raspberry pi</a><a class=tag href=/notes/arm64/>arm64</a><a class=tag href=/notes/aarch64/>aarch64</a><a class=tag href=/notes/k3s/>k3s</a><a class=tag href=/notes/kubernetes/>kubernetes</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=700 data-reading-time=3>
<header class=section-head>
<h2><a href=/notes/2019/12/the-fireside-edition/>The Fireside Edition</a></h2>
</header>
<p>After listening to the “Fireside edition” of <a href=https://changelog.com/gotime/110>Go Time</a>, I questioned myself, how would I answered the questions the hosts discussed.</p>
<p>Because no one has asked, you are very welcome:</p>
<h2 id=1-if-you-had-two-weeks-to-spend-on-a-personal-go-project-what-would-you-work-on>1. If you had two weeks to spend on a personal Go project, what would you work on?</h2>
<p>I really want to invest more time for <a href=https://github.com/profefe/profefe>profefe</a>. Specifically,
on implementing an analyser of stored profiles: the thing that would help to make sense of the data,
showing how the performance of an instance, a node, or a cluster had changed over the period of time;
how different parts of the codebase had influenced the performance of the application.</p>
<p>Recently Amazon has announced CodeGuru profiler (currently Java-only). From <a href=https://aws.amazon.com/codeguru/faqs/#Amazon_CodeGuru_Profiler>the description</a> it feels
exactly what I pictured in my head when I started the project.</p>
<p>Another topic that I would like to invest more time on is the understanding of the ecosystem around/inside Kubernetes.
During the past two years, I slowed down the consumption of the DevOps/SRE topics, mostly due to the specific
state of the infrastructure in our company. But, &ldquo;<em>k8s is the new linux</em>&rdquo;, regardless of what one&rsquo;s opinion on that.
Even profefe recently has ended up having a <a href=https://github.com/profefe/kube-profefe>kube-profefe</a> (<em>a bridge between profefe and Kubernetes</em>), contributed and maintained by other people.</p>
<h2 id=2-what-annoys-you-about-go-of-2019>2. What annoys you about Go of 2019?</h2>
<p>The same small things that annoyed me in Go 1.4: <code>var</code>, <code>new</code>, <code>make</code> and &ldquo;naked return&rdquo;. Sure, I understand that they all ended up in the language for a reason. But I simply don&rsquo;t like the &ldquo;magic&rdquo; of <code>make</code>, which works only with particular types;
the two ways of defining a variable (<code>var</code> or <code>:=</code>), or a pointer to an instance of a type (<code>new</code> or <code>&T{}</code>).</p>
<p>One new thing, though. Go modules' semver imports. But I can&rsquo;t say anything new about that. Probably, I just need to embrace them. Go 1.14 looks like a version where I might completely switch to modules, thanks to better handling of vendored dependencies.</p>
<h2 id=3-whats-your-ideal-working-environment>3. What&rsquo;s your ideal working environment?</h2>
<p>That always surprises me. Lots of people keep saying that working from home is their ideal environment or even a factor
that influence their job offers choice. I don&rsquo;t like to work at home. The only time when I feel productive when stay home is in the nights. A café or a co-working works sometimes. But I like working in a big office space. I don&rsquo;t know why.</p>
<p>Of course, open-plan offices can be very different. Yandex&rsquo;s “Red Rose” is still the best space I ever worked in. I heard they do excursions around the Moscow&rsquo;s office now.</p>
<h3 id=31-something-on-pair-programming>3.1. Something on pair-programming?</h3>
<p><em>Since I wrote about Yandex.</em></p>
<p>Some people think pair-programming is a sort of super-power. It&rsquo;s, and it&rsquo;s not.
You can&rsquo;t just put yourself in an environment, where someone is watching how you write the code while trying to hold a conversation about the code architecture. Pair-programming is a skill to master. But it pays off.</p>
<p>The pair-(trio actually)-programming sessions we did in Yandex, when we worked on <a href=https://github.com/bem/bem-core>bem-core</a>, was the most significant skill boost I had during the five-plus years there.</p>
<p>Of course, the positive experience comes from your peers. In my case, they were people with huge baggage of knowledge and practice of working, talking, debating with each other. Like, out of nowhere, you get the understanding of what
types of questions you must ask; when it is important to spend more time on thinking and when you can make a small hack.</p>
<h2 id=4-your-advice-to-you-junior-developer-self>4. Your advice to you junior-developer self?</h2>
<p>Don&rsquo;t overthink and afraid of starting anything. Trying something by making a raw, dirty, barely-working prototype will give you way more knowledge than thinking about how to do that.</p>
<footer class=meta>
<time datetime=2019-12-23T12:00:00Z>December 23, 2019</time><a class=tag href=/notes/go/>Go</a><a class=tag href=/notes/go-time-fm/>go time fm</a><a class=tag href=/notes/askme/>askme</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=400 data-reading-time=2>
<header class=section-head>
<h2><a href=/notes/2019/10/go-bytes-string-conversion/>[]byte to string conversion</a></h2>
</header>
<p>Go has an old wiki page, titled &ldquo;<a href=https://github.com/golang/go/wiki/CompilerOptimizations>Compiler And Runtime Optimizations</a>&rdquo;.</p>
<p>The part I like most there is different cases where compiler doesn&rsquo;t allocate memory for <code>string</code> to <code>[]byte</code> conversions:</p>
<blockquote>
<p>For a map m of type <code>map[string]T</code> and <code>[]byte b</code>, <code>m[string(b)]</code> doesn&rsquo;t allocate (the temporary string copy of the byte slice isn&rsquo;t made)</p>
</blockquote>
<p>Turned out, since this wiki page was written, more similar optimisations were added to the compiler.</p>
<p>As it&rsquo;s in Go 1.12+ the following cases are also listed in <a href=https://github.com/golang/go/blob/release-branch.go1.12/src/runtime/string.go#L128-L153><code>runtime/string.go</code></a>:</p>
<ul>
<li>Strings comcatenation</li>
</ul>
<p>For the case <code>"&lt;" + string(b) + ">"</code>, where <code>b</code> is <code>[]byte</code> no extra copying of <code>b</code> is needed.</p>
<ul>
<li>Comparison</li>
</ul>
<pre tabindex=0><code>if string(b) == &quot;foo&quot; { ··· }
</code></pre><p>In the code above, <code>b []byte</code> also won&rsquo;t be copied.</p>
<hr>
<p>There are still cases where compiler can&rsquo;t optimise the code for us. In some of those cases
it&rsquo;s fine to do string to bytes conversion using a so called &ldquo;<code>unsafe</code> trick&rdquo; (accessing string&rsquo;s underling
data directly, with out copying the data from string to bytes and vice versa). One can find several
ways of performing the trick, but none of them seems &ldquo;the one that must be used&rdquo;.</p>
<p>After years of episodic discussions, a collegue of mine assembled the list of different conserns and about
the proper way of doing it (<em>see &ldquo;<a href=https://groups.google.com/d/topic/golang-nuts/Zsfk-VMd_fU/discussion>unsafe conversion between string &lt;-> []byte</a>&rdquo; topic on golang-nuts forum</em>).
Thanks to replies from Go team, our most valid way of doing it is following:</p>
<pre tabindex=0><code>// Refer to github.com/fmstephe/unsafeutil

type stringHeader struct {
	data      unsafe.Pointer
	stringLen int
}

type sliceHeader struct {
	data     unsafe.Pointer
	sliceLen int
	sliceCap int
}

func StringToBytes(s string) (b []byte) {
	stringHeader := (*stringHeader)(unsafe.Pointer(&amp;s))
	sliceHeader := (*sliceHeader)(unsafe.Pointer(&amp;b))
	sliceHeader.data = stringHeader.data
	sliceHeader.sliceLen = len(s)
	sliceHeader.sliceCap = len(s)
	return b
}

func BytesToString(b []byte) (s string) {
	sliceHeader := (*sliceHeader)(unsafe.Pointer(&amp;b))
	stringHeader := (*stringHeader)(unsafe.Pointer(&amp;s))
	stringHeader.data = sliceHeader.data
	stringHeader.stringLen = len(b)
	return s
}
</code></pre>
<footer class=meta>
<time datetime=2019-10-08T12:00:00Z>October 8, 2019</time><a class=tag href=/notes/go/>Go</a><a class=tag href=/notes/%C2%B5-benchmarks/>µ-benchmarks</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=300 data-reading-time=2>
<header class=section-head>
<h2><a href=/notes/2019/09/github-actions-and-gopath/>Github Actions and GOPATH</a></h2>
</header>
<p>The other day I received my beta access to <a href=https://github.com/features/actions>GitHub Actions</a>. To try them out I picked an existing pet project and created a <em>workflow</em> using a Go project template provided by GitHub. As it&rsquo;s in <em>September 2019</em>, their template defines the sequence of steps: setup Go, checkout code, get dependencies, build. This is not exactly how I used to do it.</p>
<p>My project is a <em>classic Go service</em> ;) meaning: it uses vendoring and doesn&rsquo;t use Go modules. So no need for &ldquo;get dependencies&rdquo; step. And it requires to be inside the <code>GOPATH</code>. With that, the provided workflow needed some adjustment.</p>
<p>After some trials and errors, I&rsquo;ve managed to make <code>checkout</code> step to clone the repo into the correct destination inside the <code>GOPATH</code>. Here is the final workflow:</p>
<pre tabindex=0><code>name: Run Go test
on: [pull_request]
jobs:
  test:
    strategy:
      matrix:
        go-version: [1.12.9]

    runs-on: ubuntu-latest

    steps:
      - uses: actions/setup-go@v1
        with:
          go-version: ${{ matrix.go-version }}

      - uses: actions/checkout@v1
        with:
          path: ./src/github.com/${{ github.repository }}
          fetch-depth: 5

      - run: make test
        env:
          GOPATH: ${{ runner.workspace }}
</code></pre><p>Note, how <code>actions/checkout@v1</code> above uses custom <code>path</code> input parameter. I set the path to <code>./src/github.com/${{ github.repository }}</code>, so the project is checked out to <code>src</code> directory in the runners&rsquo;s workspace, which I later pass as the value of <code>GOPATH</code> to the &ldquo;make test&rdquo; step. The leading dot in <code>./src</code> seems very important — I&rsquo;ve spent the majority of the time trying to figure out that part — refer to <a href=https://github.com/actions/checkout/issues/41>this issue</a>.</p>
<p><a href=https://github.com/profefe/profefe/actions>See the workflow in action</a>.</p>
<p>To learn more about those <code>${{ ··· }}</code> &ldquo;macroses&rdquo; I suggest looking at the Actions' &ldquo;<a href=https://help.github.com/en/articles/contexts-and-expression-syntax-for-github-actions>Contexts and expression syntax</a>&rdquo; documentation.</p>
<footer class=meta>
<time datetime=2019-09-19T12:00:00Z>September 19, 2019</time><a class=tag href=/notes/go/>Go</a><a class=tag href=/notes/github-actions/>github actions</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=400 data-reading-time=2>
<header class=section-head>
<h2><a href=/notes/2019/09/go-http-headers/>Go's net/http.Headers</a></h2>
</header>
<p>One probably knows that <code>net/http.Headers</code> is no more than <code>map[string][]string</code> with extra specific methods. A usual way to initialize and populate such data-structure from an external representation is something like that:</p>
<pre tabindex=0><code>type Header map[string][]string

func (h Header) Add(key, val string) {
    if val == &quot;&quot; {
        return
    }
    h[key] = append(h[key], val)
}

func main() {
    h := make(Header)
    h.Add(&quot;Host&quot;, &quot;example.com&quot;)
    h.Add(&quot;Via&quot;, &quot;a.example.com&quot;)
    h.Add(&quot;Via&quot;, &quot;b.example.com&quot;)
}
</code></pre><p>From the code above, one can notice that we allocated a new slice of strings for every unique key that we added to headers. For things like HTTP headers, that&rsquo;re automatically parsed for every incoming request, this bunch of tiny allocations is something we&rsquo;d like to avoid.</p>
<p>I was curious to know if Go&rsquo;s standard library cares about that.</p>
<p>Looking at the implementation of <a href=https://golang.org/pkg/net/textproto/#Reader.ReadMIMEHeader><code>net/textproto.Reader.ReadMIMEHeader()</code></a>, which is used in the standard
HTTP server, or Go 1.13’s new <a href=https://golang.org/pkg/net/http/#Header.Clone><code>net/http.Header.Clone()</code></a>, it turned out they solve the problem quite elegantly.</p>
<p>We know that for a majority of cases, HTTP headers are an immutable key-value pair, where most of the keys have a single value. Instead of allocating a separate slice for a unique key, Go pre-allocates a continues slice for values and refers to a sub-slice of this slice for all keys.</p>
<p>Knowing that, we can refactor the initial <code>Header.Add</code> as the following:</p>
<pre tabindex=0><code>type Header map[string][]string

func (h Header) add(vv []string, key, val string) []string {
    if val == &quot;&quot; { ··· }

    // fast path for KV pair of a single value
    if h[key] == nil {
        vv = append(vv, value)
        h[key] = vv[:1:1]
        return vv[1:]
    }

    // slow path, when KV pair has two or more values
    h[key] = append(h[key], val)
    return vv
}

func main() {
    h := make(Header)
    // net/textprotocol pre-counts total number of request's headers
    // to allocate the slice of known capacity
    vv := make([]string, 0)

    vv = h.add(vv, &quot;Host&quot;, &quot;example.com&quot;)
    vv = h.add(vv, &quot;Via&quot;, &quot;a.example.com&quot;)
}
</code></pre><p>Note that we use <code>vv[:1:1]</code> to create a <a href=https://golang.org/ref/spec#Slice_expressions>sub-slice of a fixed capacity</a> (length 1, capacity 1).</p>
<p>If there is a KV-pair that has several values, e.g. &ldquo;Via&rdquo; header, <code>Add</code> will allocate a separate slice for that key, doubling its capacity.</p>
<footer class=meta>
<time datetime=2019-09-04T12:00:00Z>September 4, 2019</time><a class=tag href=/notes/go/>Go</a><a class=tag href=/notes/%C2%B5-benchmarks/>µ-benchmarks</a>
</footer>
</article>
<article class=article data-weight=0 data-word-count=100 data-reading-time=1>
<header class=section-head>
<h2><a href=/notes/2019/09/hello-world/>Hello World</a></h2>
</header>
<p>Let&rsquo;s create a blog. But let&rsquo;s call them &ldquo;notes&rdquo;.</p>
<p>Because sometimes there are thoughts I want to share with you. Some of them might even be larger than a tweet.</p>
<footer class=meta>
<time datetime=2019-09-01T12:00:00Z>September 1, 2019</time>
</footer>
</article>
</main>
<footer class=site-footer>© 2019&#8230;2022 #SocialDistancing<br><a href=mailto:vladimir@varank.in>vladimir@varank.in</a></footer>
<img src=https://c.varank.in/hello/*https://vladimir.varank.in/notes/ style=position:absolute;left:-9999px decoding=async alt>
</body>
</html>